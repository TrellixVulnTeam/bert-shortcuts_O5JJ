{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from uuid import uuid4\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "\n",
    "#role = sagemaker.get_execution_role()\n",
    "role=\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20190118T115449\".format(account_id)\n",
    "max_runs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = \"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_examples_dir = os.path.join(temp_dir, \"hugging_face_example\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup image and instance type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image_name=f\"huggingface-pytorch-training:1.9.1-transformers4.12.3-gpu-py38-cu111-ubuntu20.04\"\n",
    "image_account_id=\"763104351884\"\n",
    "instance_type = \"ml.p3.2xlarge\"\n",
    "instance_type_gpu_map = {\"ml.p3.8xlarge\":4, \"ml.p3.2xlarge\": 1, \"ml.p3.16xlarge\":8}\n",
    "instance_count=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = \"{}.dkr.ecr.{}.amazonaws.com/{}\".format(image_account_id, region, custom_image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure train/ test and validation datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"aegovan-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert=\"s3://{}/embeddings/bert_base_cased/\".format(bucket)\n",
    "\n",
    "\n",
    "trainfile = \"s3://{}/glue_dataset/train/multinli_1.0_train.jsonl\".format(bucket)\n",
    "# valfile=\"s3://{}/mnli_dataset/val/multinli_1.0_dev_matched.jsonl\".format(bucket)\n",
    "\n",
    "#trainfile = \"s3://{}/mnli_dataset_mini/train/multinli.jsonl\".format(bucket)\n",
    "valfile=\"s3://{}/glue_dataset_mini/train/multinli.jsonl\".format(bucket)\n",
    "\n",
    "s3_output_path= \"s3://{}/glue_sagemakerresults/\".format(bucket)\n",
    "s3_code_path= \"s3://{}/glue_code\".format(bucket)\n",
    "s3_checkpoint = \"s3://{}/mnli_bert_checkpoint/{}\".format(bucket, str(uuid4()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run processing job training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(transformer_examples_dir):\n",
    "    shutil.rmtree(transformer_examples_dir)\n",
    "    os.makedirs(transformer_examples_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'temp/hugging_face_example'...\n",
      "remote: Enumerating objects: 99654, done.\u001b[K\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
      "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
      "remote: Total 99654 (delta 7), reused 17 (delta 5), pack-reused 99631\u001b[K\n",
      "Receiving objects: 100% (99654/99654), 84.61 MiB | 2.65 MiB/s, done.\n",
      "Resolving deltas: 100% (72298/72298), done.\n",
      "Note: switching to 'tags/v4.12.3'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 3ea15d278 Style\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers $transformer_examples_dir\n",
    "!git -C $transformer_examples_dir checkout tags/v4.12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "script_processor = FrameworkProcessor(HuggingFace,\n",
    "                                      framework_version=None,\n",
    "                                      image_uri=docker_repo,\n",
    "                                      code_location = s3_code_path, \n",
    "                                       py_version=\"py36\",\n",
    "                                       command=[\"python\"],\n",
    "                                       env={'mode': 'python', 'PYTHONPATH':'/opt/ml/code'},\n",
    "                                       role=role,\n",
    "                                       instance_type=instance_type,\n",
    "                                       instance_count=instance_count,\n",
    "                                       max_runtime_in_seconds= 5 * 24 * 60 * 60,\n",
    "                                       volume_size_in_gb = 250,\n",
    "                                       network_config=NetworkConfig(enable_network_isolation=False),\n",
    "                                       base_job_name =\"glue-processing\"\n",
    "                                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sm_local_input_models = \"/opt/ml/processing/input/data/models\"\n",
    "sm_local_input_data = \"/opt/ml/processing/input/data/jsonlines\"\n",
    "sm_local_input_vocab = \"/opt/ml/processing/input/data/vocab\"\n",
    "\n",
    "\n",
    "sm_local_output = \"/opt/ml/processing/output\"\n",
    "\n",
    "\n",
    "\n",
    "# python run_glue.py \\\n",
    "#   --model_name_or_path bert-base-cased \\\n",
    "#   --task_name $TASK_NAME \\\n",
    "#   --do_train \\\n",
    "#   --do_eval \\\n",
    "#   --max_seq_length 128 \\\n",
    "#   --per_device_train_batch_size 32 \\\n",
    "#   --learning_rate 2e-5 \\\n",
    "#   --num_train_epochs 3 \\\n",
    "#   --output_dir /tmp/$TASK_NAME/\n",
    "\n",
    "\n",
    "script_processor.run(\n",
    "        code=f'run_glue.py',\n",
    "        source_dir=f'{transformer_examples_dir}/examples/pytorch/text-classification',\n",
    "        arguments=[\n",
    "            \"--task_name\", \"mnli\",\n",
    "            \"--model_name_or_path\", \"bert-base-cased\",\n",
    "            \"--do_train\", \"1\",\n",
    "            \"--do_eval\",\"1\",\n",
    "            \"--do_predict\",\"1\",\n",
    "            \"--max_seq_length\", str(512),\n",
    "            \"--per_device_train_batch_size\", str(8),\n",
    "            \"--gradient_accumulation_steps\", str(4),\n",
    "            \"--learning_rate\", str(2e-5),\n",
    "            \"--num_train_epochs\", str(3),\n",
    "            \"--output_dir\", sm_local_output\n",
    "           \n",
    "        ],\n",
    "\n",
    "        inputs=[\n",
    "#                 ProcessingInput(\n",
    "#                     source=s3_input_data,\n",
    "#                     s3_data_type = s3_data_type,\n",
    "#                     destination=sm_local_input_data,\n",
    "#                     s3_data_distribution_type=\"FullyReplicated\"),\n",
    "\n",
    "#                 ProcessingInput(\n",
    "#                         source=s3_model_path,\n",
    "#                         destination=sm_local_input_models,\n",
    "#                         s3_data_distribution_type=\"FullyReplicated\"),\n",
    "\n",
    "#                 ProcessingInput(\n",
    "#                         source=s3_input_vocab,\n",
    "#                         destination=sm_local_input_vocab,\n",
    "#                         s3_data_distribution_type=\"FullyReplicated\")\n",
    "            ],\n",
    "\n",
    "\n",
    "        outputs=[ProcessingOutput(\n",
    "                source=sm_local_output, \n",
    "                destination=s3_output_path,\n",
    "                output_name='predictions')]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from uuid import uuid4\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "sagemaker_session = sagemaker.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "\n",
    "#role = sagemaker.get_execution_role()\n",
    "role=\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20190118T115449\".format(account_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE_MODEL=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = \"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_examples_dir = os.path.join(temp_dir, \"hugging_face_example\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup image and instance type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image_name=f\"huggingface-pytorch-training:1.9.1-transformers4.12.3-gpu-py38-cu111-ubuntu20.04\"\n",
    "image_account_id=\"763104351884\"\n",
    "instance_type = \"ml.p3.2xlarge\"\n",
    "instance_type_gpu_map = {\"ml.p3.8xlarge\":4, \"ml.p3.2xlarge\": 1, \"ml.p3.16xlarge\":8}\n",
    "instance_count=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = \"{}.dkr.ecr.{}.amazonaws.com/{}\".format(image_account_id, region, custom_image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure train/ test and validation datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"aegovan-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert=\"s3://{}/embeddings/bert_base_cased/\".format(bucket)\n",
    "\n",
    "\n",
    "trainfile = \"s3://{}/glue_data/train.tsv\".format(bucket)\n",
    "\n",
    "val_matched_file=\"s3://{}/glue_data_pred/dev_matched.csv\".format(bucket)\n",
    "val_matched_shuffle_file=\"s3://{}/glue_data_pred/dev_matched_shuffled.csv\".format(bucket)\n",
    "\n",
    "\n",
    "val_mismatched_file=\"s3://{}/glue_data_pred/dev_mismatched.csv\".format(bucket)\n",
    "\n",
    "\n",
    "model_version = \"mnli-reverse-lang-bert-accuracy-2022-01-23-21-29-34-327\"\n",
    "\n",
    "s3_model_path = f\"s3://aegovan-data/mnli_sagemakerresults/{model_version}/output/model.tar.gz\"\n",
    "s3_model_package_path = f\"s3://aegovan-data/models/{model_version}/output\"\n",
    "s3_model_config_vocab_path = \"s3://aegovan-data/embeddings/bert_base_cased/\"\n",
    "\n",
    "s3_output_path= f\"s3://{bucket}/gluebenchmark_sagemakerresults/\"\n",
    "s3_code_path= f\"s3://{bucket}/gbucket_code\"\n",
    "s3_checkpoint = \"s3://{}/mnli_bert_checkpoint/{}\".format(bucket, str(uuid4()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run processing job training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(transformer_examples_dir):\n",
    "    shutil.rmtree(transformer_examples_dir)\n",
    "    os.makedirs(transformer_examples_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'temp/hugging_face_example'...\n",
      "remote: Enumerating objects: 101038, done.\u001B[K\n",
      "remote: Total 101038 (delta 0), reused 0 (delta 0), pack-reused 101038\u001B[K\n",
      "Receiving objects: 100% (101038/101038), 86.53 MiB | 1.30 MiB/s, done.\n",
      "Resolving deltas: 100% (73350/73350), done.\n",
      "Note: switching to 'tags/v4.12.3'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 3ea15d278 Style\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers $transformer_examples_dir\n",
    "!git -C $transformer_examples_dir checkout tags/v4.12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "framework_processor = FrameworkProcessor(HuggingFace,\n",
    "                                      framework_version=None,\n",
    "                                      image_uri=docker_repo,\n",
    "                                      code_location = s3_code_path, \n",
    "                                       py_version=\"py36\",\n",
    "                                       command=[\"python\"],\n",
    "                                       env={'mode': 'python', 'PYTHONPATH':'/opt/ml/code'},\n",
    "                                       role=role,\n",
    "                                       instance_type=instance_type,\n",
    "                                       instance_count=instance_count,\n",
    "                                       max_runtime_in_seconds= 5 * 24 * 60 * 60,\n",
    "                                       volume_size_in_gb = 250,\n",
    "                                       network_config=NetworkConfig(enable_network_isolation=False)\n",
    "                                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run base mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  gluebenchmark-bertbase-202202201547\n",
      "Inputs:  [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/gbucket_code/gluebenchmark-bertbase-202202201547/source/sourcedir.tar.gz', 'LocalPath': '/opt/ml/processing/input/code/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'entrypoint', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/gbucket_code/gluebenchmark-bertbase-202202201547/source/runproc.sh', 'LocalPath': '/opt/ml/processing/input/entrypoint', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'predictions', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://aegovan-data/gluebenchmark_sagemakerresults/gluebenchmark-bertbase-202202201547', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sm_local_input_models = \"/opt/ml/processing/input/data/models\"\n",
    "sm_local_input_data = \"/opt/ml/processing/input/data/jsonlines\"\n",
    "sm_local_input_vocab = \"/opt/ml/processing/input/data/vocab\"\n",
    "\n",
    "\n",
    "sm_local_output = \"/opt/ml/processing/output\"\n",
    "\n",
    "job_name = \"gluebenchmark-bertbase-{}\".format(datetime.now().strftime(\"%Y%m%d%H%M\"))\n",
    "\n",
    "framework_processor.run(\n",
    "        job_name = job_name,\n",
    "        wait=False,\n",
    "        code=f'run_glue.py',\n",
    "        source_dir=f'{transformer_examples_dir}/examples/pytorch/text-classification',\n",
    "        arguments=[\n",
    "            \"--task_name\", \"mnli\",\n",
    "            \"--model_name_or_path\", \"bert-base-cased\",\n",
    "            \"--do_train\", \"1\",\n",
    "            \"--do_eval\",\"1\",\n",
    "            \"--do_predict\",\"1\",\n",
    "            \"--max_seq_length\", str(512),\n",
    "            \"--per_device_train_batch_size\", str(8),\n",
    "            \"--gradient_accumulation_steps\", str(4),\n",
    "            \"--learning_rate\", str(2e-5),\n",
    "            \"--num_train_epochs\", str(3),\n",
    "            \"--output_dir\", sm_local_output,\n",
    "            \"--overwrite_output_dir\", \"1\",\n",
    "            \"--load_best_model_at_end\", \"1\",     # load the best model when finished training (default metric is loss)\n",
    "            \"--eval_steps\",\"200\",\n",
    "            \"--save_steps\",\"200\",\n",
    "            \"--evaluation_strategy\",\"steps\",\n",
    "            \"--disable_tqdm\",\"1\"\n",
    "           \n",
    "        ],\n",
    "\n",
    "        inputs=[\n",
    "#                 ProcessingInput(\n",
    "#                     source=s3_input_data,\n",
    "#                     s3_data_type = s3_data_type,\n",
    "#                     destination=sm_local_input_data,\n",
    "#                     s3_data_distribution_type=\"FullyReplicated\"),\n",
    "\n",
    "#                 ProcessingInput(\n",
    "#                         source=s3_model_path,\n",
    "#                         destination=sm_local_input_models,\n",
    "#                         s3_data_distribution_type=\"FullyReplicated\"),\n",
    "\n",
    "#                 ProcessingInput(\n",
    "#                         source=s3_input_vocab,\n",
    "#                         destination=sm_local_input_vocab,\n",
    "#                         s3_data_distribution_type=\"FullyReplicated\")\n",
    "            ],\n",
    "\n",
    "\n",
    "        outputs=[ProcessingOutput(\n",
    "                source=sm_local_output, \n",
    "                destination=\"{}/{}\".format(s3_output_path.rstrip(\"/\"), job_name) ,\n",
    "                output_name='predictions')]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with reverse train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "sm_local_input_model = \"/opt/ml/processing/input/data/model\"\n",
    "sm_local_input_config_vocab = \"/opt/ml/processing/input/data/config_vocab\"\n",
    "sm_local_output = \"/opt/ml/processing/output\"\n",
    "\n",
    "if PACKAGE_MODEL:\n",
    "    framework_processor = FrameworkProcessor(HuggingFace,\n",
    "                                      framework_version=None,\n",
    "                                      image_uri=docker_repo,\n",
    "                                       code_location = s3_code_path, \n",
    "                                       py_version=\"py36\",\n",
    "                                       command=[\"python\"],\n",
    "                                       env={'mode': 'python', 'PYTHONPATH':'/opt/ml/code'},\n",
    "                                       role=role,\n",
    "                                       instance_type=\"ml.m5.large\",\n",
    "                                       instance_count=1,\n",
    "                                       max_runtime_in_seconds= 5 * 24 * 60 * 60,\n",
    "                                       volume_size_in_gb = 250,\n",
    "                                       network_config=NetworkConfig(enable_network_isolation=False),\n",
    "                                       base_job_name =\"model-packaging\"\n",
    "                                       )\n",
    "    \n",
    "    framework_processor.run(\n",
    "            code=f'model_package_bert_utils.py',\n",
    "            source_dir=f'../src/utils',\n",
    "            arguments=[\n",
    "                \"--modeltarfile\", f\"{sm_local_input_model}/model.tar.gz\" ,\n",
    "                \"--modelconfigfile\", f\"{sm_local_input_config_vocab}/config.json\",\n",
    "                \"--vocabfile\",f\"{sm_local_input_config_vocab}/vocab.txt\",\n",
    "                \"--outdir\",sm_local_output\n",
    "\n",
    "            ],\n",
    "\n",
    "            inputs=[\n",
    "                    ProcessingInput(\n",
    "                        source=s3_model_path,\n",
    "                        s3_data_type = \"S3Prefix\",\n",
    "                        destination=sm_local_input_model,\n",
    "                        s3_data_distribution_type=\"FullyReplicated\"),\n",
    "\n",
    "                    ProcessingInput(\n",
    "                            source=s3_model_config_vocab_path,\n",
    "                            destination=sm_local_input_config_vocab,\n",
    "                            s3_data_distribution_type=\"FullyReplicated\")\n",
    "\n",
    "                ],\n",
    "\n",
    "\n",
    "            outputs=[ProcessingOutput(\n",
    "                    source=sm_local_output, \n",
    "                    destination=s3_model_package_path,\n",
    "                    output_name='predictions')]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with reverse mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  gluebenchmark-reversemnli-202202271646\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/models/mnli-reverse-lang-bert-accuracy-2022-01-23-21-29-34-327/output', 'LocalPath': '/opt/ml/processing/input/data/model', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/gbucket_code/gluebenchmark-reversemnli-202202271646/source/sourcedir.tar.gz', 'LocalPath': '/opt/ml/processing/input/code/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'entrypoint', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/gbucket_code/gluebenchmark-reversemnli-202202271646/source/runproc.sh', 'LocalPath': '/opt/ml/processing/input/entrypoint', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'predictions', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://aegovan-data/gluebenchmark_sagemakerresults/gluebenchmark-reversemnli-202202271646', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "framework_processor = FrameworkProcessor(HuggingFace,\n",
    "                                      framework_version=None,\n",
    "                                      image_uri=docker_repo,\n",
    "                                      code_location = s3_code_path, \n",
    "                                       py_version=\"py36\",\n",
    "                                       command=[\"python\"],\n",
    "                                       env={'mode': 'python', 'PYTHONPATH':'/opt/ml/code'},\n",
    "                                       role=role,\n",
    "                                       instance_type=instance_type,\n",
    "                                       instance_count=instance_count,\n",
    "                                       max_runtime_in_seconds= 5 * 24 * 60 * 60,\n",
    "                                       volume_size_in_gb = 250,\n",
    "                                       network_config=NetworkConfig(enable_network_isolation=False)\n",
    "                                       )\n",
    "\n",
    "\n",
    "\n",
    "job_name = \"gluebenchmark-reversemnli-{}\".format(datetime.now().strftime(\"%Y%m%d%H%M\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sm_local_input_model = \"/opt/ml/processing/input/data/model\"\n",
    "sm_local_input_data = \"/opt/ml/processing/input/data/jsonlines\"\n",
    "sm_local_input_vocab = \"/opt/ml/processing/input/data/vocab\"\n",
    "\n",
    "\n",
    "sm_local_output = \"/opt/ml/processing/output\"\n",
    "\n",
    "\n",
    "\n",
    "framework_processor.run(\n",
    "        job_name = job_name,\n",
    "        wait=False,\n",
    "        code=f'run_glue.py',\n",
    "    \n",
    "        source_dir=f'{transformer_examples_dir}/examples/pytorch/text-classification',\n",
    "        arguments=[\n",
    "            \"--task_name\", \"mnli\",\n",
    "            \"--model_name_or_path\", sm_local_input_model,\n",
    "            \"--do_train\", \"1\",\n",
    "            \"--do_eval\",\"1\",\n",
    "            \"--do_predict\",\"1\",\n",
    "            \"--max_seq_length\", str(512),\n",
    "            \"--per_device_train_batch_size\", str(8),\n",
    "            \"--gradient_accumulation_steps\", str(4),\n",
    "            \"--learning_rate\", str(2e-5),\n",
    "            \"--num_train_epochs\", str(10),\n",
    "            \"--output_dir\", sm_local_output,\n",
    "            \"--overwrite_output_dir\", \"1\",\n",
    "            \"--load_best_model_at_end\", \"1\",     # load the best model when finished training (default metric is loss)\n",
    "            \"--eval_steps\",\"200\",\n",
    "            \"--save_steps\",\"200\",\n",
    "            \"--evaluation_strategy\",\"steps\",\n",
    "            \"--disable_tqdm\",\"1\"\n",
    "           \n",
    "        ],\n",
    "\n",
    "        inputs=[\n",
    "#                 ProcessingInput(\n",
    "#                     source=s3_input_data,\n",
    "#                     s3_data_type = s3_data_type,\n",
    "#                     destination=sm_local_input_data,\n",
    "#                     s3_data_distribution_type=\"FullyReplicated\"),\n",
    "\n",
    "                ProcessingInput(\n",
    "                        source=s3_model_package_path,\n",
    "                        destination=sm_local_input_model,\n",
    "                        s3_data_distribution_type=\"FullyReplicated\"),\n",
    "\n",
    "#                 ProcessingInput(\n",
    "#                         source=s3_input_vocab,\n",
    "#                         destination=sm_local_input_vocab,\n",
    "#                         s3_data_distribution_type=\"FullyReplicated\")\n",
    "            ],\n",
    "\n",
    "\n",
    "        outputs=[ProcessingOutput(\n",
    "                source=sm_local_output, \n",
    "                destination=\"{}/{}\".format(s3_output_path.rstrip(\"/\"), job_name),\n",
    "                output_name='predictions')]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_experiments = {   \n",
    "     \"glue-reverse-mnli-pretrained-pred-dev-m\" : {\n",
    "        \"model\" : s3_model_package_path,\n",
    "         \"data\" : val_matched_file\n",
    "    },\n",
    "     \"glue-reverse-mnli-finetuned-pred-dev-m\" : {\n",
    "        \"model\" : \"s3://aegovan-data/gluebenchmark_sagemakerresults/gluebenchmark-reversemnli-202202201547/\",\n",
    "        \"data\" : val_matched_file\n",
    "    },\n",
    "    \"glue-bert-base-pretrained-pred-dev-m\" : {\n",
    "        \"model\" : pretrained_bert,\n",
    "        \"data\" : val_matched_file\n",
    "    },\n",
    "     \"glue-bert-base-finetuned-pred-dev-m\" : {\n",
    "        \"model\" : \"s3://aegovan-data/gluebenchmark_sagemakerresults/gluebenchmark-bertbase-202202201547/\",\n",
    "         \"data\" : val_matched_file\n",
    "    },\n",
    "    \"glue-reverse-mnli-pretrained-pred-dev-m-shuf\" : {\n",
    "        \"model\" : s3_model_package_path,\n",
    "         \"data\" : val_matched_shuffle_file\n",
    "    },\n",
    "    \"glue-reverse-mnli-finetuned-pred-dev-m-shuf\" : {\n",
    "        \"model\" : \"s3://aegovan-data/gluebenchmark_sagemakerresults/gluebenchmark-reversemnli-202202201547/\",\n",
    "        \"data\" : val_matched_shuffle_file\n",
    "    },\n",
    "    \"glue-bert-base-pretrained-pred-dev-m-shuf\" : {\n",
    "        \"model\" : pretrained_bert,\n",
    "        \"data\" : val_matched_shuffle_file\n",
    "    },\n",
    "     \"glue-bert-base-finetuned-pred-dev-m-shuf\" : {\n",
    "        \"model\" : \"s3://aegovan-data/gluebenchmark_sagemakerresults/gluebenchmark-bertbase-202202201547/\",\n",
    "        \"data\" : val_matched_shuffle_file\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"glue-bert-base-finetuned-pred-dev-m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  glue-bert-base-finetuned-pred-dev-m-202202271506\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/glue_data_pred/dev_matched.csv', 'LocalPath': '/opt/ml/processing/input/data/jsonlines', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/gluebenchmark_sagemakerresults/gluebenchmark-bertbase-202202201547/', 'LocalPath': '/opt/ml/processing/input/data/model', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/gbucket_code/glue-bert-base-finetuned-pred-dev-m-202202271506/source/sourcedir.tar.gz', 'LocalPath': '/opt/ml/processing/input/code/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'entrypoint', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/gbucket_code/glue-bert-base-finetuned-pred-dev-m-202202271506/source/runproc.sh', 'LocalPath': '/opt/ml/processing/input/entrypoint', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'predictions', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://aegovan-data/gluebenchmark_sagemakerresults/glue-bert-base-finetuned-pred-dev-m-202202271506', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "framework_processor = FrameworkProcessor(HuggingFace,\n",
    "                                      framework_version=None,\n",
    "                                      image_uri=docker_repo,\n",
    "                                      code_location = s3_code_path, \n",
    "                                       py_version=\"py36\",\n",
    "                                       command=[\"python\"],\n",
    "                                       env={'mode': 'python', 'PYTHONPATH':'/opt/ml/code'},\n",
    "                                       role=role,\n",
    "                                       instance_type=instance_type,\n",
    "                                       instance_count=instance_count,\n",
    "                                       max_runtime_in_seconds= 5 * 24 * 60 * 60,\n",
    "                                       volume_size_in_gb = 250,\n",
    "                                       network_config=NetworkConfig(enable_network_isolation=False)\n",
    "                                       )\n",
    "\n",
    "job_name = \"{}-{}\".format(experiment, datetime.now().strftime(\"%Y%m%d%H%M\"))\n",
    "s3_inference_model = inference_experiments[experiment][\"model\"]\n",
    "prediction_s3_file = inference_experiments[experiment][\"data\"]\n",
    "\n",
    "sm_local_input_model = \"/opt/ml/processing/input/data/model\"\n",
    "sm_local_input_data = \"/opt/ml/processing/input/data/jsonlines\"\n",
    "sm_local_input_vocab = \"/opt/ml/processing/input/data/vocab\"\n",
    "\n",
    "sm_local_output = \"/opt/ml/processing/output\"\n",
    "\n",
    "predict_local_file = \"{}/{}\".format(sm_local_input_data.rstrip(\"/\"),prediction_s3_file.split(\"/\")[-1] )\n",
    "framework_processor.run(\n",
    "        job_name = job_name,\n",
    "        wait=False,\n",
    "        code=f'run_glue.py',\n",
    "    \n",
    "        source_dir=f'{transformer_examples_dir}/examples/pytorch/text-classification',\n",
    "        arguments=[\n",
    "           # \"--task_name\", \"mnli\",\n",
    "            \"--model_name_or_path\", sm_local_input_model,\n",
    "            \"--do_train\", \"0\",\n",
    "            \"--do_eval\",\"1\",\n",
    "            \"--do_predict\",\"1\",\n",
    "            \"--train_file\", predict_local_file,\n",
    "            \"--validation_file\", predict_local_file,\n",
    "            \"--test_file\", predict_local_file,\n",
    "            \"--max_seq_length\", str(512),\n",
    "            \"--per_device_train_batch_size\", str(32),\n",
    "        \n",
    "            \"--output_dir\", sm_local_output,\n",
    "           \n",
    "            \n",
    "            \"--disable_tqdm\",\"1\"\n",
    "           \n",
    "        ],\n",
    "\n",
    "        inputs=[\n",
    "                ProcessingInput(\n",
    "                    source=prediction_s3_file,\n",
    "                    destination=sm_local_input_data,\n",
    "                    s3_data_distribution_type=\"FullyReplicated\"),\n",
    "\n",
    "                ProcessingInput(\n",
    "                        source=s3_inference_model,\n",
    "                        destination=sm_local_input_model,\n",
    "                        s3_data_distribution_type=\"FullyReplicated\"),\n",
    "\n",
    "#                 ProcessingInput(\n",
    "#                         source=s3_input_vocab,\n",
    "#                         destination=sm_local_input_vocab,\n",
    "#                         s3_data_distribution_type=\"FullyReplicated\")\n",
    "            ],\n",
    "\n",
    "\n",
    "        outputs=[ProcessingOutput(\n",
    "                source=sm_local_output, \n",
    "                destination=\"{}/{}\".format(s3_output_path.rstrip(\"/\"), job_name),\n",
    "                output_name='predictions')]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
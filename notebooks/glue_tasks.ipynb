{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from uuid import uuid4\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "\n",
    "#role = sagemaker.get_execution_role()\n",
    "role=\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20190118T115449\".format(account_id)\n",
    "max_runs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = \"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_examples_dir = os.path.join(temp_dir, \"hugging_face_example\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup image and instance type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image_name=f\"huggingface-pytorch-training:1.9.1-transformers4.12.3-gpu-py38-cu111-ubuntu20.04\"\n",
    "image_account_id=\"763104351884\"\n",
    "instance_type = \"ml.p3.2xlarge\"\n",
    "instance_type_gpu_map = {\"ml.p3.8xlarge\":4, \"ml.p3.2xlarge\": 1, \"ml.p3.16xlarge\":8}\n",
    "instance_count=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = \"{}.dkr.ecr.{}.amazonaws.com/{}\".format(image_account_id, region, custom_image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure train/ test and validation datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"aegovan-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert=\"s3://{}/embeddings/bert_base_cased/\".format(bucket)\n",
    "\n",
    "\n",
    "trainfile = \"s3://{}/glue_dataset/train/multinli_1.0_train.jsonl\".format(bucket)\n",
    "# valfile=\"s3://{}/mnli_dataset/val/multinli_1.0_dev_matched.jsonl\".format(bucket)\n",
    "\n",
    "#trainfile = \"s3://{}/mnli_dataset_mini/train/multinli.jsonl\".format(bucket)\n",
    "valfile=\"s3://{}/glue_dataset_mini/train/multinli.jsonl\".format(bucket)\n",
    "\n",
    "s3_output_path= \"s3://{}/glue_sagemakerresults/\".format(bucket)\n",
    "s3_code_path= \"s3://{}/glue_code\".format(bucket)\n",
    "s3_checkpoint = \"s3://{}/mnli_bert_checkpoint/{}\".format(bucket, str(uuid4()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run processing job training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(transformer_examples_dir):\n",
    "    shutil.rmtree(transformer_examples_dir)\n",
    "    os.makedirs(transformer_examples_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'temp/hugging_face_example'...\n",
      "remote: Enumerating objects: 99654, done.\u001b[K\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
      "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
      "remote: Total 99654 (delta 7), reused 17 (delta 5), pack-reused 99631\u001b[K\n",
      "Receiving objects: 100% (99654/99654), 84.61 MiB | 2.65 MiB/s, done.\n",
      "Resolving deltas: 100% (72298/72298), done.\n",
      "Note: switching to 'tags/v4.12.3'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 3ea15d278 Style\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers $transformer_examples_dir\n",
    "!git -C $transformer_examples_dir checkout tags/v4.12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "script_processor = FrameworkProcessor(HuggingFace,\n",
    "                                      framework_version=None,\n",
    "                                      image_uri=docker_repo,\n",
    "                                      code_location = s3_code_path, \n",
    "                                       py_version=\"py36\",\n",
    "                                       command=[\"python\"],\n",
    "                                       env={'mode': 'python', 'PYTHONPATH':'/opt/ml/code'},\n",
    "                                       role=role,\n",
    "                                       instance_type=instance_type,\n",
    "                                       instance_count=instance_count,\n",
    "                                       max_runtime_in_seconds= 5 * 24 * 60 * 60,\n",
    "                                       volume_size_in_gb = 250,\n",
    "                                       network_config=NetworkConfig(enable_network_isolation=False),\n",
    "                                       base_job_name =\"glue-processing\"\n",
    "                                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  glue-processing-2022-02-12-18-55-12-257\n",
      "Inputs:  [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/glue_code/glue-processing-2022-02-12-18-55-12-257/source/sourcedir.tar.gz', 'LocalPath': '/opt/ml/processing/input/code/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'entrypoint', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/glue_code/glue-processing-2022-02-12-18-55-12-257/source/runproc.sh', 'LocalPath': '/opt/ml/processing/input/entrypoint', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'predictions', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://aegovan-data/glue_sagemakerresults/', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "...................................\u001b[34mWARNING: Skipping typing as it is not installed.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mCollecting accelerate\n",
      "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.8.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (0.1.96)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (0.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (3.19.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from accelerate->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from accelerate->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2021.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.62.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (5.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 5)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets>=1.8.0->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (5.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: accelerate\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.5.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:14 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:14 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=True,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=True,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_steps=200,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=4,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=False,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_strategy=HubStrategy.EVERY_SAVE,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=2e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=True,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=-1,\u001b[0m\n",
      "\u001b[34mlog_level_replica=-1,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/processing/output/runs/Feb12_19-01-13_ip-10-0-129-205.us-east-2.compute.internal,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=loss,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3.0,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/processing/output,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=True,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=8,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/processing/output,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=200,\u001b[0m\n",
      "\u001b[34msave_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:14 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.15.1/datasets/glue/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpya5orvk6\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s]#015Downloading: 28.8kB [00:00, 19.0MB/s]                   \u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:14 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.15.1/datasets/glue/glue.py in cache at /root/.cache/huggingface/datasets/downloads/ebcebc40af3c6b9af1a2f380ea06637ee192bce2d17d528809dd0ee2fa281675.759f3e257a3fad0984d9f8ba9a26479d341795eb50fa64e4c1de40f1fc421313.py\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:14 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/ebcebc40af3c6b9af1a2f380ea06637ee192bce2d17d528809dd0ee2fa281675.759f3e257a3fad0984d9f8ba9a26479d341795eb50fa64e4c1de40f1fc421313.py\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:14 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.15.1/datasets/glue/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmplcf6js2e\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]#015Downloading: 28.7kB [00:00, 23.9MB/s]                   \u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:14 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.15.1/datasets/glue/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/eea1163a0dd089739f6e5e3951d74b1200c7e66d462d607c43cbc8c4e69bbd47.082d8848abcb8cddda90647ec069014ca338abd4f45e0a83c6df1ece0d45476a\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:14 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/eea1163a0dd089739f6e5e3951d74b1200c7e66d462d607c43cbc8c4e69bbd47.082d8848abcb8cddda90647ec069014ca338abd4f45e0a83c6df1ece0d45476a\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:14 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:14 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset glue/mnli (download: 298.29 MiB, generated: 78.65 MiB, post-processed: Unknown size, total: 376.95 MiB) to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:14 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:14 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/MNLI.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp2c2aj8v_\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015Downloading:   0%|          | 0.00/313M [00:00<?, ?B/s]#015Downloading:   0%|          | 9.22k/313M [00:00<57:04, 91.3kB/s]#015Downloading:   0%|          | 54.3k/313M [00:00<17:51, 292kB/s] #015Downloading:   0%|          | 124k/313M [00:00<11:02, 472kB/s] #015Downloading:   0%|          | 298k/313M [00:00<05:24, 964kB/s]#015Downloading:   0%|          | 625k/313M [00:00<02:54, 1.79MB/s]#015Downloading:   0%|          | 1.26M/313M [00:00<01:34, 3.29MB/s]#015Downloading:   1%|          | 2.53M/313M [00:00<00:49, 6.31MB/s]#015Downloading:   2%|▏         | 5.08M/313M [00:00<00:25, 12.2MB/s]#015Downloading:   3%|▎         | 9.18M/313M [00:00<00:14, 21.0MB/s]#015Downloading:   4%|▍         | 12.7M/313M [00:01<00:11, 25.4MB/s]#015Downloading:   5%|▌         | 16.7M/313M [00:01<00:10, 29.0MB/s]#015Downloading:   7%|▋         | 20.9M/313M [00:01<00:09, 31.7MB/s]#015Downloading:   8%|▊         | 25.0M/313M [00:01<00:08, 34.6MB/s]#015Downloading:   9%|▉         | 28.5M/313M [00:01<00:08, 33.4MB/s]#015Downloading:  11%|█         | 33.0M/313M [00:01<00:07, 36.7MB/s]#015Downloading:  12%|█▏        | 36.7M/313M [00:01<00:07, 35.6MB/s]#015Downloading:  13%|█▎        | 41.0M/313M [00:01<00:07, 37.8MB/s]#015Downloading:  15%|█▍        | 45.4M/313M [00:01<00:06, 39.6MB/s]#015Downloading:  16%|█▌        | 49.4M/313M [00:01<00:06, 39.3MB/s]#015Downloading:  17%|█▋        | 53.3M/313M [00:02<00:06, 39.2MB/s]#015Downloading:  18%|█▊        | 57.3M/313M [00:02<00:06, 39.1MB/s]#015Downloading:  20%|█▉        | 61.4M/313M [00:02<00:06, 39.9MB/s]#015Downloading:  21%|██        | 65.5M/313M [00:02<00:06, 40.2MB/s]#015Downloading:  22%|██▏       | 69.5M/313M [00:02<00:06, 37.9MB/s]#015Downloading:  24%|██▍       | 74.5M/313M [00:02<00:05, 41.4MB/s]#015Downloading:  25%|██▌       | 78.7M/313M [00:02<00:05, 41.5MB/s]#015Downloading:  27%|██▋       | 82.9M/313M [00:02<00:05, 41.1MB/s]#015Downloading:  28%|██▊       | 87.0M/313M [00:02<00:05, 40.8MB/s]#015Downloading:  29%|██▉       | 91.1M/313M [00:02<00:05, 40.6MB/s]#015Downloading:  30%|███       | 95.2M/313M [00:03<00:05, 40.5MB/s]#015Downloading:  32%|███▏      | 99.2M/313M [00:03<00:05, 40.2MB/s]#015Downloading:  33%|███▎      | 103M/313M [00:03<00:05, 39.4MB/s] #015Downloading:  34%|███▍      | 107M/313M [00:03<00:05, 38.7MB/s]#015Downloading:  36%|███▌      | 112M/313M [00:03<00:04, 40.7MB/s]#015Downloading:  37%|███▋      | 116M/313M [00:03<00:04, 40.5MB/s]#015Downloading:  38%|███▊      | 120M/313M [00:03<00:04, 40.9MB/s]#015Downloading:  40%|███▉      | 124M/313M [00:03<00:04, 38.7MB/s]#015Downloading:  41%|████▏     | 129M/313M [00:03<00:04, 42.8MB/s]#015Downloading:  43%|████▎     | 134M/313M [00:04<00:04, 41.7MB/s]#015Downloading:  44%|████▍     | 138M/313M [00:04<00:04, 42.7MB/s]#015Downloading:  46%|████▌     | 143M/313M [00:04<00:04, 42.3MB/s]#015Downloading:  47%|████▋     | 147M/313M [00:04<00:04, 37.9MB/s]#015Downloading:  48%|████▊     | 151M/313M [00:04<00:04, 38.8MB/s]#015Downloading:  50%|████▉     | 155M/313M [00:04<00:03, 39.4MB/s]#015Downloading:  51%|█████     | 159M/313M [00:04<00:03, 39.7MB/s]#015Downloading:  52%|█████▏    | 164M/313M [00:04<00:03, 40.0MB/s]#015Downloading:  54%|█████▎    | 168M/313M [00:04<00:03, 40.4MB/s]#015Downloading:  55%|█████▍    | 172M/313M [00:04<00:03, 40.7MB/s]#015Downloading:  56%|█████▋    | 176M/313M [00:05<00:03, 40.5MB/s]#015Downloading:  58%|█████▊    | 180M/313M [00:05<00:03, 40.3MB/s]#015Downloading:  59%|█████▉    | 184M/313M [00:05<00:03, 40.4MB/s]#015Downloading:  60%|██████    | 188M/313M [00:05<00:03, 40.3MB/s]#015Downloading:  61%|██████▏   | 192M/313M [00:05<00:02, 40.5MB/s]#015Downloading:  63%|██████▎   | 196M/313M [00:05<00:02, 40.2MB/s]#015Downloading:  64%|██████▍   | 201M/313M [00:05<00:02, 40.3MB/s]#015Downloading:  65%|██████▌   | 205M/313M [00:05<00:02, 40.6MB/s]#015Downloading:  67%|██████▋   | 209M/313M [00:05<00:02, 39.0MB/s]#015Downloading:  68%|██████▊   | 213M/313M [00:06<00:02, 41.0MB/s]#015Downloading:  70%|██████▉   | 218M/313M [00:06<00:02, 40.8MB/s]#015Downloading:  71%|███████   | 222M/313M [00:06<00:02, 40.9MB/s]#015Downloading:  72%|███████▏  | 226M/313M [00:06<00:02, 41.3MB/s]#015Downloading:  74%|███████▎  | 230M/313M [00:06<00:01, 41.5MB/s]#015Downloading:  75%|███████▍  | 234M/313M [00:06<00:01, 41.2MB/s]#015Downloading:  76%|███████▌  | 238M/313M [00:06<00:01, 40.9MB/s]#015Downloading:  78%|███████▊  | 243M/313M [00:06<00:01, 39.8MB/s]#015Downloading:  79%|███████▉  | 247M/313M [00:06<00:01, 39.6MB/s]#015Downloading:  80%|████████  | 251M/313M [00:06<00:01, 41.6MB/s]#015Downloading:  82%|████████▏ | 255M/313M [00:07<00:01, 40.6MB/s]#015Downloading:  83%|████████▎ | 260M/313M [00:07<00:01, 41.4MB/s]#015Downloading:  84%|████████▍ | 264M/313M [00:07<00:01, 40.7MB/s]#015Downloading:  86%|████████▌ | 268M/313M [00:07<00:01, 41.0MB/s]#015Downloading:  87%|████████▋ | 272M/313M [00:07<00:00, 40.8MB/s]#015Downloading:  88%|████████▊ | 276M/313M [00:07<00:00, 40.5MB/s]#015Downloading:  90%|████████▉ | 280M/313M [00:07<00:00, 40.6MB/s]#015Downloading:  91%|█████████ | 284M/313M [00:07<00:00, 40.7MB/s]#015Downloading:  92%|█████████▏| 289M/313M [00:07<00:00, 40.7MB/s]#015Downloading:  94%|█████████▎| 293M/313M [00:07<00:00, 40.9MB/s]#015Downloading:  95%|█████████▍| 297M/313M [00:08<00:00, 38.4MB/s]#015Downloading:  96%|█████████▋| 302M/313M [00:08<00:00, 41.6MB/s]#015Downloading:  98%|█████████▊| 306M/313M [00:08<00:00, 41.7MB/s]#015Downloading:  99%|█████████▉| 310M/313M [00:08<00:00, 41.6MB/s]#015Downloading: 100%|██████████| 313M/313M [00:08<00:00, 37.1MB/s]\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:23 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/MNLI.zip in cache at /root/.cache/huggingface/datasets/downloads/74d7bc70ada44c1086d1ba81cf6271c128514f629fb8edcd548c113939e3b5f2\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:23 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/74d7bc70ada44c1086d1ba81cf6271c128514f629fb8edcd548c113939e3b5f2\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:23 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:24 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:32 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:32 - INFO - datasets.builder - Generating split train\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:51 - INFO - datasets.builder - Generating split validation_matched\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:52 - INFO - datasets.builder - Generating split validation_mismatched\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:52 - INFO - datasets.builder - Generating split test_matched\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:53 - INFO - datasets.builder - Generating split test_mismatched\u001b[0m\n",
      "\u001b[34m02/12/2022 19:01:53 - INFO - datasets.utils.info_utils - All the splits matched successfully.\u001b[0m\n",
      "\u001b[34mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m#0150 examples [00:00, ? examples/s]#0151954 examples [00:00, 19532.33 examples/s]#0154044 examples [00:00, 20335.90 examples/s]#0156133 examples [00:00, 20585.38 examples/s]#0158217 examples [00:00, 20683.34 examples/s]#01510286 examples [00:00, 19765.39 examples/s]#01512433 examples [00:00, 20326.73 examples/s]#01514578 examples [00:00, 20685.25 examples/s]#01516692 examples [00:00, 20826.97 examples/s]#01518792 examples [00:00, 20876.16 examples/s]#01520883 examples [00:01, 20325.25 examples/s]#01523028 examples [00:01, 20660.18 examples/s]#01525153 examples [00:01, 20834.37 examples/s]#01527257 examples [00:01, 20893.91 examples/s]#01529371 examples [00:01, 20964.84 examples/s]#01531470 examples [00:01, 18001.14 examples/s]#01533527 examples [00:01, 18690.52 examples/s]#01535626 examples [00:01, 19326.85 examples/s]#01537783 examples [00:01, 19962.59 examples/s]#01539922 examples [00:01, 20372.50 examples/s]#01541987 examples [00:02, 20108.84 examples/s]#01544162 examples [00:02, 20585.03 examples/s]#01546300 examples [00:02, 20816.47 examples/s]#01548470 examples [00:02, 21075.32 examples/s]#01550586 examples [00:02, 20538.19 examples/s]#01552772 examples [00:02, 20923.13 examples/s]#01554959 examples [00:02, 21200.12 examples/s]#01557143 examples [00:02, 21386.90 examples/s]#01559304 examples [00:02, 21450.79 examples/s]#01561452 examples [00:03, 20901.96 examples/s]#01563627 examples [00:03, 21147.36 examples/s]#01565816 examples [00:03, 21363.59 examples/s]#01567976 examples [00:03, 21432.21 examples/s]#01570122 examples [00:03, 20741.36 examples/s]#01572298 examples [00:03, 21034.73 examples/s]#01574477 examples [00:03, 21255.80 examples/s]#01576653 examples [00:03, 21401.91 examples/s]#01578816 examples [00:03, 21468.52 examples/s]#01580966 examples [00:03, 20905.10 examples/s]#01583061 examples [00:04, 18738.36 examples/s]#01585229 examples [00:04, 19538.32 examples/s]#01587400 examples [00:04, 20144.45 examples/s]#01589554 examples [00:04, 20541.26 examples/s]#01591632 examples [00:04, 20197.89 examples/s]#01593819 examples [00:04, 20676.69 examples/s]#01596004 examples [00:04, 21016.60 examples/s]#01598187 examples [00:04, 21255.72 examples/s]#015100321 examples [00:04, 20664.20 examples/s]#015102517 examples [00:04, 21040.01 examples/s]#015104687 examples [00:05, 21233.03 examples/s]#015106850 examples [00:05, 21348.88 examples/s]#015109010 examples [00:05, 21420.42 examples/s]#015111155 examples [00:05, 20860.10 examples/s]#015113344 examples [00:05, 21161.30 examples/s]#015115516 examples [00:05, 21325.28 examples/s]#015117686 examples [00:05, 21435.86 examples/s]#015119843 examples [00:05, 21473.03 examples/s]#015121992 examples [00:05, 20880.00 examples/s]#015124178 examples [00:05, 21164.65 examples/s]#015126353 examples [00:06, 21333.58 examples/s]#015128525 examples [00:06, 21446.62 examples/s]#015130672 examples [00:06, 20810.95 examples/s]#015132855 examples [00:06, 21105.74 examples/s]#015135050 examples [00:06, 21353.84 examples/s]#015137190 examples [00:06, 19129.30 examples/s]#015139344 examples [00:06, 19790.74 examples/s]#015141361 examples [00:06, 19627.97 examples/s]#015143543 examples [00:06, 20248.27 examples/s]#015145727 examples [00:07, 20705.99 examples/s]#015147902 examples [00:07, 21008.87 examples/s]#015150016 examples [00:07, 20507.49 examples/s]#015152206 examples [00:07, 20909.50 examples/s]#015154383 examples [00:07, 21159.84 examples/s]#015156554 examples [00:07, 21321.72 examples/s]#015158722 examples [00:07, 21426.43 examples/s]#015160869 examples [00:07, 20809.73 examples/s]#015163041 examples [00:07, 21073.91 examples/s]#015165229 examples [00:07, 21310.24 examples/s]#015167421 examples [00:08, 21488.73 examples/s]#015169602 examples [00:08, 21582.68 examples/s]#015171763 examples [00:08, 21008.00 examples/s]#015173948 examples [00:08, 21253.46 examples/s]#015176132 examples [00:08, 21423.15 examples/s]#015178307 examples [00:08, 21518.08 examples/s]#015180461 examples [00:08, 20818.77 examples/s]#015182652 examples [00:08, 21135.21 examples/s]#015184847 examples [00:08, 21373.32 examples/s]#015187040 examples [00:08, 21537.33 examples/s]#015189197 examples [00:09, 19136.49 examples/s]#015191162 examples [00:09, 19227.93 examples/s]#015193344 examples [00:09, 19952.42 examples/s]#015195544 examples [00:09, 20536.11 examples/s]#015197724 examples [00:09, 20901.87 examples/s]#015199894 examples [00:09, 21135.30 examples/s]#015202021 examples [00:09, 20634.93 examples/s]#015204218 examples [00:09, 21023.26 examples/s]#015206411 examples [00:09, 21288.97 examples/s]#015208598 examples [00:10, 21459.92 examples/s]#015210750 examples [00:10, 20888.73 examples/s]#015212944 examples [00:10, 21193.97 examples/s]#015215133 examples [00:10, 21395.29 examples/s]#015217322 examples [00:10, 21537.22 examples/s]#015219502 examples [00:10, 21614.31 examples/s]#015221666 examples [00:10, 21010.74 examples/s]#015223843 examples [00:10, 21232.68 examples/s]#015226030 examples [00:10, 21419.31 examples/s]#015228213 examples [00:10, 21538.11 examples/s]#015230370 examples [00:11, 20850.05 examples/s]#015232558 examples [00:11, 21148.07 examples/s]#015234727 examples [00:11, 21306.75 examples/s]#015236913 examples [00:11, 21469.53 examples/s]#015239104 examples [00:11, 21600.00 examples/s]#015241267 examples [00:11, 20990.44 examples/s]#015243436 examples [00:11, 18884.06 examples/s]#015245574 examples [00:11, 19560.45 examples/s]#015247704 examples [00:11, 20044.65 examples/s]#015249851 examples [00:12, 20450.90 examples/s]#015251920 examples [00:12, 20053.38 examples/s]#015254115 examples [00:12, 20598.82 examples/s]#015256305 examples [00:12, 20977.20 examples/s]#015258488 examples [00:12, 21225.63 examples/s]#015260619 examples [00:12, 20659.51 examples/s]#015262799 examples [00:12, 20990.05 examples/s]#015264990 examples [00:12, 21258.36 examples/s]#015267174 examples [00:12, 21427.12 examples/s]#015269367 examples [00:12, 21573.03 examples/s]#015271528 examples [00:13, 21002.72 examples/s]#015273735 examples [00:13, 21313.06 examples/s]#015275924 examples [00:13, 21481.05 examples/s]#015278100 examples [00:13, 21562.58 examples/s]#015280259 examples [00:13, 20926.35 examples/s]#015282451 examples [00:13, 21213.91 examples/s]#015284647 examples [00:13, 21432.06 examples/s]#015286849 examples [00:13, 21604.92 examples/s]#015289037 examples [00:13, 21686.29 examples/s]#015291208 examples [00:13, 21072.60 examples/s]#015293393 examples [00:14, 21297.49 examples/s]#015295572 examples [00:14, 21439.32 examples/s]#015297719 examples [00:14, 19066.96 examples/s]#015299880 examples [00:14, 19760.26 examples/s]#015301898 examples [00:14, 19663.51 examples/s]#015304059 examples [00:14, 20215.80 examples/s]#015306246 examples [00:14, 20691.92 examples/s]#015308416 examples [00:14, 20984.87 examples/s]#015310528 examples [00:14, 20535.86 examples/s]#015312724 examples [00:15, 20949.19 examples/s]#015314909 examples [00:15, 21211.46 examples/s]#015317099 examples [00:15, 21411.59 examples/s]#015319260 examples [00:15, 21469.98 examples/s]#015321411 examples [00:15, 20788.84 examples/s]#015323598 examples [00:15, 21101.77 examples/s]#015325787 examples [00:15, 21332.01 examples/s]#015327973 examples [00:15, 21486.27 examples/s]#015330125 examples [00:15, 20874.10 examples/s]#015332317 examples [00:15, 21176.38 examples/s]#015334487 examples [00:16, 21329.50 examples/s]#015336624 examples [00:16, 20417.23 examples/s]#015338676 examples [00:16, 19512.39 examples/s]#015340640 examples [00:16, 19407.29 examples/s]#015342802 examples [00:16, 20038.90 examples/s]#015344970 examples [00:16, 20510.95 examples/s]#015347137 examples [00:16, 20847.95 examples/s]#015349229 examples [00:16, 18159.96 examples/s]#015351150 examples [00:16, 18440.67 examples/s]#015353329 examples [00:17, 19366.69 examples/s]#015355481 examples [00:17, 19975.73 examples/s]#015357655 examples [00:17, 20482.66 examples/s]#015359832 examples [00:17, 20856.09 examples/s]#015361937 examples [00:17, 20484.26 examples/s]#015364116 examples [00:17, 20864.06 examples/s]#015366284 examples [00:17, 21101.85 examples/s]#015368457 examples [00:17, 21286.31 examples/s]#015370592 examples [00:17, 20761.47 examples/s]#015372782 examples [00:17, 21091.80 examples/s]#015374951 examples [00:18, 21265.66 examples/s]#015377139 examples [00:18, 21444.92 examples/s]#015379322 examples [00:18, 21558.71 examples/s]#015381481 examples [00:18, 20921.79 examples/s]#015383683 examples [00:18, 21242.14 examples/s]#015385854 examples [00:18, 21379.00 examples/s]#015388043 examples [00:18, 21530.10 examples/s]#015390199 examples [00:18, 20864.71 examples/s]#015392402 examples [00:18, 21203.17 examples/s]#015                                            #015#0150 examples [00:00, ? examples/s]#0152059 examples [00:00, 20584.57 examples/s]#0154200 examples [00:00, 21067.42 examples/s]#0156351 examples [00:00, 21264.31 examples/s]#0158478 examples [00:00, 18017.73 examples/s]#015                                          #015#0150 examples [00:00, ? examples/s]#0152050 examples [00:00, 20491.86 examples/s]#0154179 examples [00:00, 20956.62 examples/s]#0156314 examples [00:00, 21133.57 examples/s]#0158438 examples [00:00, 21171.58 examples/s]#015                                          #015#0150 examples [00:00, ? examples/s]#0152278 examples [00:00, 22771.66 examples/s]#0154646 examples [00:00, 23299.41 examples/s]#0156980 examples [00:00, 23315.01 examples/s]#0159346 examples [00:00, 23449.75 examples/s]#015                                          #015#0150 examples [00:00, ? examples/s]#0152234 examples [00:00, 22336.83 examples/s]#0154581 examples [00:00, 22998.47 examples/s]#0156881 examples [00:00, 22881.22 examples/s]#0159231 examples [00:00, 23121.79 examples/s]#015                                          #015#015  0%|          | 0/5 [00:00<?, ?it/s]#015100%|██████████| 5/5 [00:00<00:00, 849.74it/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1753] 2022-02-12 19:01:53,554 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkltf6l5x\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 570/570 [00:00<00:00, 839kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1757] 2022-02-12 19:01:53,621 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1765] 2022-02-12 19:01:53,621 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:588] 2022-02-12 19:01:53,621 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:625] 2022-02-12 19:01:53,622 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1753] 2022-02-12 19:01:53,688 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpy7mhu9_3\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 46.5kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1757] 2022-02-12 19:01:53,753 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1765] 2022-02-12 19:01:53,753 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:588] 2022-02-12 19:01:53,819 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:625] 2022-02-12 19:01:53,820 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1753] 2022-02-12 19:01:53,958 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8chvopj6\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 208k/208k [00:00<00:00, 6.24MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1757] 2022-02-12 19:01:54,059 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1765] 2022-02-12 19:01:54,059 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1753] 2022-02-12 19:01:54,126 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmps21hwmv5\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 426k/426k [00:00<00:00, 7.85MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1757] 2022-02-12 19:01:54,253 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1765] 2022-02-12 19:01:54,253 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1742] 2022-02-12 19:01:54,449 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1742] 2022-02-12 19:01:54,449 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1742] 2022-02-12 19:01:54,449 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1742] 2022-02-12 19:01:54,449 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1742] 2022-02-12 19:01:54,449 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:588] 2022-02-12 19:01:54,515 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:625] 2022-02-12 19:01:54,516 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1753] 2022-02-12 19:01:54,637 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp21apz5a0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]#015Downloading:   0%|          | 937k/416M [00:00<00:45, 9.59MB/s]#015Downloading:   2%|▏         | 6.27M/416M [00:00<00:11, 37.0MB/s]#015Downloading:   2%|▏         | 10.2M/416M [00:00<00:10, 39.1MB/s]#015Downloading:   3%|▎         | 14.2M/416M [00:00<00:10, 40.2MB/s]#015Downloading:   4%|▍         | 18.1M/416M [00:00<00:11, 35.5MB/s]#015Downloading:   5%|▌         | 21.5M/416M [00:00<00:12, 32.6MB/s]#015Downloading:   6%|▌         | 25.5M/416M [00:00<00:11, 35.1MB/s]#015Downloading:   7%|▋         | 29.9M/416M [00:00<00:10, 38.5MB/s]#015Downloading:   8%|▊         | 33.7M/416M [00:00<00:10, 36.5MB/s]#015Downloading:   9%|▉         | 37.2M/416M [00:01<00:11, 34.4MB/s]#015Downloading:  10%|▉         | 41.2M/416M [00:01<00:10, 36.2MB/s]#015Downloading:  11%|█         | 45.6M/416M [00:01<00:09, 39.2MB/s]#015Downloading:  12%|█▏        | 49.4M/416M [00:01<00:10, 37.0MB/s]#015Downloading:  13%|█▎        | 53.3M/416M [00:01<00:10, 37.9MB/s]#015Downloading:  14%|█▎        | 56.9M/416M [00:01<00:11, 33.2MB/s]#015Downloading:  15%|█▍        | 60.6M/416M [00:01<00:10, 34.7MB/s]#015Downloading:  16%|█▌        | 64.5M/416M [00:01<00:10, 36.2MB/s]#015Downloading:  16%|█▋        | 68.0M/416M [00:01<00:10, 36.3MB/s]#015Downloading:  17%|█▋        | 71.6M/416M [00:02<00:10, 34.6MB/s]#015Downloading:  18%|█▊        | 75.8M/416M [00:02<00:09, 37.4MB/s]#015Downloading:  19%|█▉        | 79.5M/416M [00:02<00:09, 36.7MB/s]#015Downloading:  20%|██        | 83.7M/416M [00:02<00:08, 38.7MB/s]#015Downloading:  21%|██        | 87.4M/416M [00:02<00:09, 34.5MB/s]#015Downloading:  22%|██▏       | 91.0M/416M [00:02<00:09, 35.2MB/s]#015Downloading:  23%|██▎       | 96.1M/416M [00:02<00:08, 40.4MB/s]#015Downloading:  24%|██▍       | 100M/416M [00:02<00:09, 36.6MB/s] #015Downloading:  25%|██▍       | 104M/416M [00:03<00:09, 35.1MB/s]#015Downloading:  26%|██▌       | 107M/416M [00:03<00:10, 30.9MB/s]#015Downloading:  27%|██▋       | 110M/416M [00:03<00:11, 29.0MB/s]#015Downloading:  28%|██▊       | 115M/416M [00:03<00:09, 34.6MB/s]#015Downloading:  29%|██▊       | 119M/416M [00:03<00:09, 31.6MB/s]#015Downloading:  30%|██▉       | 124M/416M [00:03<00:08, 36.9MB/s]#015Downloading:  31%|███       | 127M/416M [00:03<00:08, 36.2MB/s]#015Downloading:  32%|███▏      | 131M/416M [00:03<00:08, 36.5MB/s]#015Downloading:  32%|███▏      | 135M/416M [00:04<00:10, 29.2MB/s]#015Downloading:  33%|███▎      | 138M/416M [00:04<00:10, 27.4MB/s]#015Downloading:  34%|███▍      | 140M/416M [00:04<00:16, 17.0MB/s]#015Downloading:  35%|███▌      | 146M/416M [00:04<00:11, 23.7MB/s]#015Downloading:  36%|███▌      | 149M/416M [00:04<00:11, 24.2MB/s]#015Downloading:  37%|███▋      | 153M/416M [00:04<00:09, 27.7MB/s]#015Downloading:  38%|███▊      | 156M/416M [00:05<00:10, 25.7MB/s]#015Downloading:  38%|███▊      | 159M/416M [00:05<00:11, 24.1MB/s]#015Downloading:  39%|███▉      | 161M/416M [00:05<00:11, 22.2MB/s]#015Downloading:  39%|███▉      | 164M/416M [00:05<00:11, 22.7MB/s]#015Downloading:  40%|███▉      | 166M/416M [00:05<00:11, 22.9MB/s]#015Downloading:  41%|████      | 168M/416M [00:05<00:11, 22.6MB/s]#015Downloading:  41%|████      | 171M/416M [00:05<00:11, 22.7MB/s]#015Downloading:  42%|████▏     | 173M/416M [00:05<00:11, 21.4MB/s]#015Downloading:  42%|████▏     | 175M/416M [00:05<00:11, 22.2MB/s]#015Downloading:  43%|████▎     | 177M/416M [00:06<00:11, 22.3MB/s]#015Downloading:  43%|████▎     | 180M/416M [00:06<00:10, 23.6MB/s]#015Downloading:  44%|████▍     | 182M/416M [00:06<00:10, 23.8MB/s]#015Downloading:  44%|████▍     | 185M/416M [00:06<00:09, 24.8MB/s]#015Downloading:  45%|████▌     | 187M/416M [00:06<00:09, 24.9MB/s]#015Downloading:  46%|████▌     | 190M/416M [00:06<00:10, 23.0MB/s]#015Downloading:  46%|████▌     | 192M/416M [00:06<00:10, 22.5MB/s]#015Downloading:  47%|████▋     | 194M/416M [00:06<00:10, 21.1MB/s]#015Downloading:  47%|████▋     | 197M/416M [00:06<00:10, 22.3MB/s]#015Downloading:  48%|████▊     | 200M/416M [00:07<00:08, 25.6MB/s]#015Downloading:  49%|████▉     | 203M/416M [00:07<00:08, 25.4MB/s]#015Downloading:  49%|████▉     | 205M/416M [00:07<00:08, 24.9MB/s]#015Downloading:  50%|████▉     | 207M/416M [00:07<00:08, 24.6MB/s]#015Downloading:  50%|█████     | 210M/416M [00:07<00:09, 22.8MB/s]#015Downloading:  51%|█████     | 213M/416M [00:07<00:08, 25.1MB/s]#015Downloading:  52%|█████▏    | 216M/416M [00:07<00:07, 26.8MB/s]#015Downloading:  53%|█████▎    | 220M/416M [00:07<00:06, 30.6MB/s]#015Downloading:  54%|█████▍    | 224M/416M [00:07<00:06, 33.5MB/s]#015Downloading:  55%|█████▌    | 229M/416M [00:08<00:04, 39.7MB/s]#015Downloading:  56%|█████▌    | 233M/416M [00:08<00:04, 38.8MB/s]#015Downloading:  57%|█████▋    | 237M/416M [00:08<00:05, 35.7MB/s]#015Downloading:  58%|█████▊    | 240M/416M [00:08<00:05, 31.3MB/s]#015Downloading:  59%|█████▊    | 243M/416M [00:08<00:05, 31.4MB/s]#015Downloading:  59%|█████▉    | 246M/416M [00:08<00:05, 29.8MB/s]#015Downloading:  60%|██████    | 250M/416M [00:08<00:05, 31.1MB/s]#015Downloading:  61%|██████    | 253M/416M [00:08<00:05, 32.8MB/s]#015Downloading:  62%|██████▏   | 257M/416M [00:08<00:04, 33.3MB/s]#015Downloading:  63%|██████▎   | 261M/416M [00:09<00:04, 36.2MB/s]#015Downloading:  64%|██████▎   | 264M/416M [00:09<00:04, 36.1MB/s]#015Downloading:  65%|██████▍   | 268M/416M [00:09<00:04, 37.3MB/s]#015Downloading:  65%|██████▌   | 272M/416M [00:09<00:04, 34.1MB/s]#015Downloading:  66%|██████▋   | 275M/416M [00:09<00:04, 34.3MB/s]#015Downloading:  67%|██████▋   | 279M/416M [00:09<00:04, 31.5MB/s]#015Downloading:  68%|██████▊   | 283M/416M [00:09<00:04, 34.6MB/s]#015Downloading:  69%|██████▉   | 287M/416M [00:09<00:03, 35.8MB/s]#015Downloading:  70%|███████   | 292M/416M [00:09<00:03, 42.2MB/s]#015Downloading:  72%|███████▏  | 298M/416M [00:10<00:02, 46.8MB/s]#015Downloading:  73%|███████▎  | 302M/416M [00:10<00:02, 46.2MB/s]#015Downloading:  74%|███████▍  | 307M/416M [00:10<00:02, 44.5MB/s]#015Downloading:  75%|███████▌  | 312M/416M [00:10<00:02, 46.5MB/s]#015Downloading:  76%|███████▌  | 316M/416M [00:10<00:02, 45.8MB/s]#015Downloading:  77%|███████▋  | 321M/416M [00:10<00:02, 40.4MB/s]#015Downloading:  78%|███████▊  | 325M/416M [00:10<00:02, 41.6MB/s]#015Downloading:  79%|███████▉  | 329M/416M [00:10<00:02, 41.7MB/s]#015Downloading:  80%|████████  | 333M/416M [00:10<00:02, 41.8MB/s]#015Downloading:  81%|████████▏ | 338M/416M [00:11<00:01, 43.7MB/s]#015Downloading:  82%|████████▏ | 342M/416M [00:11<00:01, 39.6MB/s]#015Downloading:  83%|████████▎ | 346M/416M [00:11<00:02, 34.8MB/s]#015Downloading:  84%|████████▍ | 350M/416M [00:11<00:01, 37.0MB/s]#015Downloading:  85%|████████▌ | 354M/416M [00:11<00:01, 36.9MB/s]#015Downloading:  86%|████████▌ | 357M/416M [00:11<00:01, 34.6MB/s]#015Downloading:  87%|████████▋ | 361M/416M [00:11<00:01, 34.0MB/s]#015Downloading:  88%|████████▊ | 364M/416M [00:11<00:01, 33.8MB/s]#015Downloading:  89%|████████▊ | 368M/416M [00:11<00:01, 35.9MB/s]#015Downloading:  89%|████████▉ | 371M/416M [00:12<00:01, 31.6MB/s]#015Downloading:  90%|█████████ | 374M/416M [00:12<00:01, 28.8MB/s]#015Downloading:  91%|█████████ | 378M/416M [00:12<00:01, 31.0MB/s]#015Downloading:  92%|█████████▏| 382M/416M [00:12<00:01, 33.1MB/s]#015Downloading:  93%|█████████▎| 385M/416M [00:12<00:01, 31.0MB/s]#015Downloading:  93%|█████████▎| 388M/416M [00:12<00:00, 32.3MB/s]#015Downloading:  94%|█████████▍| 392M/416M [00:12<00:00, 31.2MB/s]#015Downloading:  95%|█████████▌| 395M/416M [00:12<00:00, 32.5MB/s]#015Downloading:  96%|█████████▌| 398M/416M [00:13<00:00, 29.9MB/s]#015Downloading:  97%|█████████▋| 403M/416M [00:13<00:00, 36.2MB/s]#015Downloading:  98%|█████████▊| 407M/416M [00:13<00:00, 36.4MB/s]#015Downloading:  99%|█████████▉| 411M/416M [00:13<00:00, 38.1MB/s]#015Downloading: 100%|█████████▉| 415M/416M [00:13<00:00, 36.9MB/s]#015Downloading: 100%|██████████| 416M/416M [00:13<00:00, 32.4MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1757] 2022-02-12 19:02:08,235 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1765] 2022-02-12 19:02:08,235 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1340] 2022-02-12 19:02:08,235 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1598] 2022-02-12 19:02:09,833 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1609] 2022-02-12 19:02:09,833 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m02/12/2022 19:02:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-48413cedb88f4fab.arrow\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015Running tokenizer on dataset:   0%|          | 0/393 [00:00<?, ?ba/s]#015Running tokenizer on dataset:   0%|          | 1/393 [00:00<02:33,  2.56ba/s]#015Running tokenizer on dataset:   1%|          | 2/393 [00:00<01:40,  3.88ba/s]#015Running tokenizer on dataset:   1%|          | 3/393 [00:00<01:35,  4.08ba/s]#015Running tokenizer on dataset:   1%|          | 4/393 [00:00<01:22,  4.73ba/s]#015Running tokenizer on dataset:   1%|▏         | 5/393 [00:01<01:15,  5.16ba/s]#015Running tokenizer on dataset:   2%|▏         | 6/393 [00:01<01:11,  5.44ba/s]#015Running tokenizer on dataset:   2%|▏         | 7/393 [00:01<01:08,  5.64ba/s]#015Running tokenizer on dataset:   2%|▏         | 8/393 [00:01<01:06,  5.75ba/s]#015Running tokenizer on dataset:   2%|▏         | 9/393 [00:01<01:05,  5.88ba/s]#015Running tokenizer on dataset:   3%|▎         | 10/393 [00:01<01:04,  5.97ba/s]#015Running tokenizer on dataset:   3%|▎         | 11/393 [00:02<01:03,  5.99ba/s]#015Running tokenizer on dataset:   3%|▎         | 12/393 [00:02<01:03,  6.02ba/s]#015Running tokenizer on dataset:   3%|▎         | 13/393 [00:02<01:03,  6.02ba/s]#015Running tokenizer on dataset:   4%|▎         | 14/393 [00:02<01:02,  6.04ba/s]#015Running tokenizer on dataset:   4%|▍         | 15/393 [00:02<01:03,  6.00ba/s]#015Running tokenizer on dataset:   4%|▍         | 16/393 [00:02<01:07,  5.60ba/s]#015Running tokenizer on dataset:   4%|▍         | 17/393 [00:03<01:05,  5.78ba/s]#015Running tokenizer on dataset:   5%|▍         | 18/393 [00:03<01:03,  5.93ba/s]#015Running tokenizer on dataset:   5%|▍         | 19/393 [00:03<01:02,  6.03ba/s]#015Running tokenizer on dataset:   5%|▌         | 20/393 [00:03<01:01,  6.09ba/s]#015Running tokenizer on dataset:   5%|▌         | 21/393 [00:03<01:01,  6.09ba/s]#015Running tokenizer on dataset:   6%|▌         | 22/393 [00:03<01:00,  6.12ba/s]#015Running tokenizer on dataset:   6%|▌         | 23/393 [00:04<01:00,  6.07ba/s]#015Running tokenizer on dataset:   6%|▌         | 24/393 [00:04<01:00,  6.07ba/s]#015Running tokenizer on dataset:   6%|▋         | 25/393 [00:04<01:00,  6.06ba/s]#015Running tokenizer on dataset:   7%|▋         | 26/393 [00:04<01:00,  6.05ba/s]#015Running tokenizer on dataset:   7%|▋         | 27/393 [00:04<01:00,  6.00ba/s]#015Running tokenizer on dataset:   7%|▋         | 28/393 [00:04<01:05,  5.62ba/s]#015Running tokenizer on dataset:   7%|▋         | 29/393 [00:05<01:02,  5.81ba/s]#015Running tokenizer on dataset:   8%|▊         | 30/393 [00:05<01:00,  5.96ba/s]#015Running tokenizer on dataset:   8%|▊         | 31/393 [00:05<00:59,  6.06ba/s]#015Running tokenizer on dataset:   8%|▊         | 32/393 [00:05<00:59,  6.10ba/s]#015Running tokenizer on dataset:   8%|▊         | 33/393 [00:05<00:58,  6.11ba/s]#015Running tokenizer on dataset:   9%|▊         | 34/393 [00:05<00:58,  6.10ba/s]#015Running tokenizer on dataset:   9%|▉         | 35/393 [00:06<00:58,  6.10ba/s]#015Running tokenizer on dataset:   9%|▉         | 36/393 [00:06<00:58,  6.11ba/s]#015Running tokenizer on dataset:   9%|▉         | 37/393 [00:06<00:58,  6.11ba/s]#015Running tokenizer on dataset:  10%|▉         | 38/393 [00:06<00:57,  6.13ba/s]#015Running tokenizer on dataset:  10%|▉         | 39/393 [00:06<00:58,  6.08ba/s]#015Running tokenizer on dataset:  10%|█         | 40/393 [00:06<01:02,  5.67ba/s]#015Running tokenizer on dataset:  10%|█         | 41/393 [00:07<01:00,  5.83ba/s]#015Running tokenizer on dataset:  11%|█         | 42/393 [00:07<00:59,  5.95ba/s]#015Running tokenizer on dataset:  11%|█         | 43/393 [00:07<00:57,  6.04ba/s]#015Running tokenizer on dataset:  11%|█         | 44/393 [00:07<00:57,  6.10ba/s]#015Running tokenizer on dataset:  11%|█▏        | 45/393 [00:07<00:57,  6.09ba/s]#015Running tokenizer on dataset:  12%|█▏        | 46/393 [00:07<00:56,  6.11ba/s]#015Running tokenizer on dataset:  12%|█▏        | 47/393 [00:08<00:56,  6.11ba/s]#015Running tokenizer on dataset:  12%|█▏        | 48/393 [00:08<00:56,  6.12ba/s]#015Running tokenizer on dataset:  12%|█▏        | 49/393 [00:08<00:56,  6.10ba/s]#015Running tokenizer on dataset:  13%|█▎        | 50/393 [00:08<00:56,  6.09ba/s]#015Running tokenizer on dataset:  13%|█▎        | 51/393 [00:08<00:56,  6.09ba/s]#015Running tokenizer on dataset:  13%|█▎        | 52/393 [00:08<00:59,  5.69ba/s]#015Running tokenizer on dataset:  13%|█▎        | 53/393 [00:09<00:58,  5.85ba/s]#015Running tokenizer on dataset:  14%|█▎        | 54/393 [00:09<00:56,  6.00ba/s]#015Running tokenizer on dataset:  14%|█▍        | 55/393 [00:09<00:55,  6.07ba/s]#015Running tokenizer on dataset:  14%|█▍        | 56/393 [00:09<00:55,  6.10ba/s]#015Running tokenizer on dataset:  15%|█▍        | 57/393 [00:09<00:55,  6.10ba/s]#015Running tokenizer on dataset:  15%|█▍        | 58/393 [00:09<00:54,  6.10ba/s]#015Running tokenizer on dataset:  15%|█▌        | 59/393 [00:10<00:54,  6.11ba/s]#015Running tokenizer on dataset:  15%|█▌        | 60/393 [00:10<00:54,  6.10ba/s]#015Running tokenizer on dataset:  16%|█▌        | 61/393 [00:10<00:54,  6.07ba/s]#015Running tokenizer on dataset:  16%|█▌        | 62/393 [00:10<00:54,  6.07ba/s]#015Running tokenizer on dataset:  16%|█▌        | 63/393 [00:10<00:54,  6.06ba/s]#015Running tokenizer on dataset:  16%|█▋        | 64/393 [00:10<00:58,  5.62ba/s]#015Running tokenizer on dataset:  17%|█▋        | 65/393 [00:11<00:56,  5.80ba/s]#015Running tokenizer on dataset:  17%|█▋        | 66/393 [00:11<00:55,  5.89ba/s]#015Running tokenizer on dataset:  17%|█▋        | 67/393 [00:11<00:54,  6.00ba/s]#015Running tokenizer on dataset:  17%|█▋        | 68/393 [00:11<00:53,  6.04ba/s]#015Running tokenizer on dataset:  18%|█▊        | 69/393 [00:11<00:53,  6.08ba/s]#015Running tokenizer on dataset:  18%|█▊        | 70/393 [00:11<00:53,  6.08ba/s]#015Running tokenizer on dataset:  18%|█▊        | 71/393 [00:12<00:52,  6.09ba/s]#015Running tokenizer on dataset:  18%|█▊        | 72/393 [00:12<00:52,  6.09ba/s]#015Running tokenizer on dataset:  19%|█▊        | 73/393 [00:12<00:52,  6.09ba/s]#015Running tokenizer on dataset:  19%|█▉        | 74/393 [00:12<00:52,  6.10ba/s]#015Running tokenizer on dataset:  19%|█▉        | 75/393 [00:12<00:52,  6.10ba/s]#015Running tokenizer on dataset:  19%|█▉        | 76/393 [00:12<00:56,  5.64ba/s]#015Running tokenizer on dataset:  20%|█▉        | 77/393 [00:13<00:54,  5.82ba/s]#015Running tokenizer on dataset:  20%|█▉        | 78/393 [00:13<00:53,  5.94ba/s]#015Running tokenizer on dataset:  20%|██        | 79/393 [00:13<00:52,  6.03ba/s]#015Running tokenizer on dataset:  20%|██        | 80/393 [00:13<00:51,  6.07ba/s]#015Running tokenizer on dataset:  21%|██        | 81/393 [00:13<00:51,  6.09ba/s]#015Running tokenizer on dataset:  21%|██        | 82/393 [00:13<00:50,  6.12ba/s]#015Running tokenizer on dataset:  21%|██        | 83/393 [00:14<00:50,  6.10ba/s]#015Running tokenizer on dataset:  21%|██▏       | 84/393 [00:14<00:50,  6.07ba/s]#015Running tokenizer on dataset:  22%|██▏       | 85/393 [00:14<00:51,  6.04ba/s]#015Running tokenizer on dataset:  22%|██▏       | 86/393 [00:14<00:50,  6.03ba/s]#015Running tokenizer on dataset:  22%|██▏       | 87/393 [00:14<00:50,  6.01ba/s]#015Running tokenizer on dataset:  22%|██▏       | 88/393 [00:14<00:54,  5.61ba/s]#015Running tokenizer on dataset:  23%|██▎       | 89/393 [00:15<00:53,  5.70ba/s]#015Running tokenizer on dataset:  23%|██▎       | 90/393 [00:15<01:00,  5.02ba/s]#015Running tokenizer on dataset:  23%|██▎       | 91/393 [00:15<00:56,  5.31ba/s]#015Running tokenizer on dataset:  23%|██▎       | 92/393 [00:15<00:54,  5.53ba/s]#015Running tokenizer on dataset:  24%|██▎       | 93/393 [00:15<00:52,  5.68ba/s]#015Running tokenizer on dataset:  24%|██▍       | 94/393 [00:16<00:51,  5.79ba/s]#015Running tokenizer on dataset:  24%|██▍       | 95/393 [00:16<00:50,  5.86ba/s]#015Running tokenizer on dataset:  24%|██▍       | 96/393 [00:16<00:49,  5.94ba/s]#015Running tokenizer on dataset:  25%|██▍       | 97/393 [00:16<00:49,  5.99ba/s]#015Running tokenizer on dataset:  25%|██▍       | 98/393 [00:16<00:49,  6.01ba/s]#015Running tokenizer on dataset:  25%|██▌       | 99/393 [00:16<00:48,  6.01ba/s]#015Running tokenizer on dataset:  25%|██▌       | 100/393 [00:17<00:52,  5.63ba/s]#015Running tokenizer on dataset:  26%|██▌       | 101/393 [00:17<00:50,  5.80ba/s]#015Running tokenizer on dataset:  26%|██▌       | 102/393 [00:17<00:49,  5.92ba/s]#015Running tokenizer on dataset:  26%|██▌       | 103/393 [00:17<00:48,  6.01ba/s]#015Running tokenizer on dataset:  26%|██▋       | 104/393 [00:17<00:47,  6.06ba/s]#015Running tokenizer on dataset:  27%|██▋       | 105/393 [00:17<00:47,  6.09ba/s]#015Running tokenizer on dataset:  27%|██▋       | 106/393 [00:18<00:47,  6.08ba/s]#015Running tokenizer on dataset:  27%|██▋       | 107/393 [00:18<00:47,  6.07ba/s]#015Running tokenizer on dataset:  27%|██▋       | 108/393 [00:18<00:46,  6.09ba/s]#015Running tokenizer on dataset:  28%|██▊       | 109/393 [00:18<00:46,  6.08ba/s]#015Running tokenizer on dataset:  28%|██▊       | 110/393 [00:18<00:46,  6.08ba/s]#015Running tokenizer on dataset:  28%|██▊       | 111/393 [00:18<00:46,  6.03ba/s]#015Running tokenizer on dataset:  28%|██▊       | 112/393 [00:19<00:50,  5.55ba/s]#015Running tokenizer on dataset:  29%|██▉       | 113/393 [00:19<00:48,  5.74ba/s]#015Running tokenizer on dataset:  29%|██▉       | 114/393 [00:19<00:47,  5.86ba/s]#015Running tokenizer on dataset:  29%|██▉       | 115/393 [00:19<00:46,  5.94ba/s]#015Running tokenizer on dataset:  30%|██▉       | 116/393 [00:19<00:46,  6.00ba/s]#015Running tokenizer on dataset:  30%|██▉       | 117/393 [00:19<00:45,  6.01ba/s]#015Running tokenizer on dataset:  30%|███       | 118/393 [00:20<00:45,  6.01ba/s]#015Running tokenizer on dataset:  30%|███       | 119/393 [00:20<00:45,  5.98ba/s]#015Running tokenizer on dataset:  31%|███       | 120/393 [00:20<00:45,  5.99ba/s]#015Running tokenizer on dataset:  31%|███       | 121/393 [00:20<00:45,  6.00ba/s]#015Running tokenizer on dataset:  31%|███       | 122/393 [00:20<00:45,  6.00ba/s]#015Running tokenizer on dataset:  31%|███▏      | 123/393 [00:20<00:44,  6.01ba/s]#015Running tokenizer on dataset:  32%|███▏      | 124/393 [00:21<00:47,  5.61ba/s]#015Running tokenizer on dataset:  32%|███▏      | 125/393 [00:21<00:46,  5.81ba/s]#015Running tokenizer on dataset:  32%|███▏      | 126/393 [00:21<00:45,  5.90ba/s]#015Running tokenizer on dataset:  32%|███▏      | 127/393 [00:21<00:44,  5.99ba/s]#015Running tokenizer on dataset:  33%|███▎      | 128/393 [00:21<00:43,  6.06ba/s]#015Running tokenizer on dataset:  33%|███▎      | 129/393 [00:21<00:43,  6.07ba/s]#015Running tokenizer on dataset:  33%|███▎      | 130/393 [00:22<00:43,  6.10ba/s]#015Running tokenizer on dataset:  33%|███▎      | 131/393 [00:22<00:42,  6.11ba/s]#015Running tokenizer on dataset:  34%|███▎      | 132/393 [00:22<00:42,  6.10ba/s]#015Running tokenizer on dataset:  34%|███▍      | 133/393 [00:22<00:42,  6.09ba/s]#015Running tokenizer on dataset:  34%|███▍      | 134/393 [00:22<00:42,  6.07ba/s]#015Running tokenizer on dataset:  34%|███▍      | 135/393 [00:22<00:42,  6.04ba/s]#015Running tokenizer on dataset:  35%|███▍      | 136/393 [00:23<00:45,  5.62ba/s]#015Running tokenizer on dataset:  35%|███▍      | 137/393 [00:23<00:44,  5.81ba/s]#015Running tokenizer on dataset:  35%|███▌      | 138/393 [00:23<00:42,  5.93ba/s]#015Running tokenizer on dataset:  35%|███▌      | 139/393 [00:23<00:41,  6.05ba/s]#015Running tokenizer on dataset:  36%|███▌      | 140/393 [00:23<00:41,  6.08ba/s]#015Running tokenizer on dataset:  36%|███▌      | 141/393 [00:23<00:41,  6.10ba/s]#015Running tokenizer on dataset:  36%|███▌      | 142/393 [00:24<00:41,  6.07ba/s]#015Running tokenizer on dataset:  36%|███▋      | 143/393 [00:24<00:41,  6.07ba/s]#015Running tokenizer on dataset:  37%|███▋      | 144/393 [00:24<00:41,  6.05ba/s]#015Running tokenizer on dataset:  37%|███▋      | 145/393 [00:24<00:41,  6.03ba/s]#015Running tokenizer on dataset:  37%|███▋      | 146/393 [00:24<00:40,  6.04ba/s]#015Running tokenizer on dataset:  37%|███▋      | 147/393 [00:24<00:40,  6.02ba/s]#015Running tokenizer on dataset:  38%|███▊      | 148/393 [00:25<00:43,  5.63ba/s]#015Running tokenizer on dataset:  38%|███▊      | 149/393 [00:25<00:42,  5.80ba/s]#015Running tokenizer on dataset:  38%|███▊      | 150/393 [00:25<00:41,  5.93ba/s]#015Running tokenizer on dataset:  38%|███▊      | 151/393 [00:25<00:40,  6.03ba/s]#015Running tokenizer on dataset:  39%|███▊      | 152/393 [00:25<00:39,  6.09ba/s]#015Running tokenizer on dataset:  39%|███▉      | 153/393 [00:25<00:39,  6.09ba/s]#015Running tokenizer on dataset:  39%|███▉      | 154/393 [00:26<00:39,  6.12ba/s]#015Running tokenizer on dataset:  39%|███▉      | 155/393 [00:26<00:38,  6.12ba/s]#015Running tokenizer on dataset:  40%|███▉      | 156/393 [00:26<00:38,  6.12ba/s]#015Running tokenizer on dataset:  40%|███▉      | 157/393 [00:26<00:38,  6.08ba/s]#015Running tokenizer on dataset:  40%|████      | 158/393 [00:26<00:38,  6.08ba/s]#015Running tokenizer on dataset:  40%|████      | 159/393 [00:26<00:38,  6.01ba/s]#015Running tokenizer on dataset:  41%|████      | 160/393 [00:27<00:41,  5.63ba/s]#015Running tokenizer on dataset:  41%|████      | 161/393 [00:27<00:40,  5.79ba/s]#015Running tokenizer on dataset:  41%|████      | 162/393 [00:27<00:39,  5.90ba/s]#015Running tokenizer on dataset:  41%|████▏     | 163/393 [00:27<00:38,  5.97ba/s]#015Running tokenizer on dataset:  42%|████▏     | 164/393 [00:27<00:37,  6.04ba/s]#015Running tokenizer on dataset:  42%|████▏     | 165/393 [00:27<00:37,  6.06ba/s]#015Running tokenizer on dataset:  42%|████▏     | 166/393 [00:28<00:37,  6.10ba/s]#015Running tokenizer on dataset:  42%|████▏     | 167/393 [00:28<00:37,  6.09ba/s]#015Running tokenizer on dataset:  43%|████▎     | 168/393 [00:28<00:36,  6.10ba/s]#015Running tokenizer on dataset:  43%|████▎     | 169/393 [00:28<00:36,  6.07ba/s]#015Running tokenizer on dataset:  43%|████▎     | 170/393 [00:28<00:36,  6.07ba/s]#015Running tokenizer on dataset:  44%|████▎     | 171/393 [00:28<00:36,  6.03ba/s]#015Running tokenizer on dataset:  44%|████▍     | 172/393 [00:29<00:39,  5.64ba/s]#015Running tokenizer on dataset:  44%|████▍     | 173/393 [00:29<00:37,  5.84ba/s]#015Running tokenizer on dataset:  44%|████▍     | 174/393 [00:29<00:36,  5.94ba/s]#015Running tokenizer on dataset:  45%|████▍     | 175/393 [00:29<00:36,  6.01ba/s]#015Running tokenizer on dataset:  45%|████▍     | 176/393 [00:29<00:35,  6.06ba/s]#015Running tokenizer on dataset:  45%|████▌     | 177/393 [00:29<00:35,  6.07ba/s]#015Running tokenizer on dataset:  45%|████▌     | 178/393 [00:30<00:35,  6.07ba/s]#015Running tokenizer on dataset:  46%|████▌     | 179/393 [00:30<00:35,  6.06ba/s]#015Running tokenizer on dataset:  46%|████▌     | 180/393 [00:30<00:35,  6.08ba/s]#015Running tokenizer on dataset:  46%|████▌     | 181/393 [00:30<00:38,  5.44ba/s]#015Running tokenizer on dataset:  46%|████▋     | 182/393 [00:30<00:38,  5.53ba/s]#015Running tokenizer on dataset:  47%|████▋     | 183/393 [00:30<00:37,  5.59ba/s]#015Running tokenizer on dataset:  47%|████▋     | 184/393 [00:31<00:39,  5.30ba/s]#015Running tokenizer on dataset:  47%|████▋     | 185/393 [00:31<00:37,  5.54ba/s]#015Running tokenizer on dataset:  47%|████▋     | 186/393 [00:31<00:36,  5.70ba/s]#015Running tokenizer on dataset:  48%|████▊     | 187/393 [00:31<00:35,  5.81ba/s]#015Running tokenizer on dataset:  48%|████▊     | 188/393 [00:31<00:34,  5.91ba/s]#015Running tokenizer on dataset:  48%|████▊     | 189/393 [00:31<00:34,  5.96ba/s]#015Running tokenizer on dataset:  48%|████▊     | 190/393 [00:32<00:33,  6.01ba/s]#015Running tokenizer on dataset:  49%|████▊     | 191/393 [00:32<00:33,  6.05ba/s]#015Running tokenizer on dataset:  49%|████▉     | 192/\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m393 [00:32<00:33,  6.06ba/s]#015Running tokenizer on dataset:  49%|████▉     | 193/393 [00:32<00:32,  6.06ba/s]#015Running tokenizer on dataset:  49%|████▉     | 194/393 [00:32<00:32,  6.08ba/s]#015Running tokenizer on dataset:  50%|████▉     | 195/393 [00:32<00:32,  6.05ba/s]#015Running tokenizer on dataset:  50%|████▉     | 196/393 [00:33<00:34,  5.68ba/s]#015Running tokenizer on dataset:  50%|█████     | 197/393 [00:33<00:33,  5.84ba/s]#015Running tokenizer on dataset:  50%|█████     | 198/393 [00:33<00:32,  5.97ba/s]#015Running tokenizer on dataset:  51%|█████     | 199/393 [00:33<00:32,  6.04ba/s]#015Running tokenizer on dataset:  51%|█████     | 200/393 [00:33<00:31,  6.08ba/s]#015Running tokenizer on dataset:  51%|█████     | 201/393 [00:33<00:31,  6.11ba/s]#015Running tokenizer on dataset:  51%|█████▏    | 202/393 [00:34<00:31,  6.11ba/s]#015Running tokenizer on dataset:  52%|█████▏    | 203/393 [00:34<00:31,  6.10ba/s]#015Running tokenizer on dataset:  52%|█████▏    | 204/393 [00:34<00:31,  6.08ba/s]#015Running tokenizer on dataset:  52%|█████▏    | 205/393 [00:34<00:31,  6.05ba/s]#015Running tokenizer on dataset:  52%|█████▏    | 206/393 [00:34<00:30,  6.05ba/s]#015Running tokenizer on dataset:  53%|█████▎    | 207/393 [00:34<00:31,  5.98ba/s]#015Running tokenizer on dataset:  53%|█████▎    | 208/393 [00:35<00:33,  5.57ba/s]#015Running tokenizer on dataset:  53%|█████▎    | 209/393 [00:35<00:31,  5.75ba/s]#015Running tokenizer on dataset:  53%|█████▎    | 210/393 [00:35<00:31,  5.90ba/s]#015Running tokenizer on dataset:  54%|█████▎    | 211/393 [00:35<00:30,  5.99ba/s]#015Running tokenizer on dataset:  54%|█████▍    | 212/393 [00:35<00:29,  6.07ba/s]#015Running tokenizer on dataset:  54%|█████▍    | 213/393 [00:35<00:29,  6.09ba/s]#015Running tokenizer on dataset:  54%|█████▍    | 214/393 [00:36<00:29,  6.12ba/s]#015Running tokenizer on dataset:  55%|█████▍    | 215/393 [00:36<00:29,  6.12ba/s]#015Running tokenizer on dataset:  55%|█████▍    | 216/393 [00:36<00:29,  6.10ba/s]#015Running tokenizer on dataset:  55%|█████▌    | 217/393 [00:36<00:28,  6.09ba/s]#015Running tokenizer on dataset:  55%|█████▌    | 218/393 [00:36<00:28,  6.08ba/s]#015Running tokenizer on dataset:  56%|█████▌    | 219/393 [00:36<00:28,  6.05ba/s]#015Running tokenizer on dataset:  56%|█████▌    | 220/393 [00:37<00:30,  5.64ba/s]#015Running tokenizer on dataset:  56%|█████▌    | 221/393 [00:37<00:29,  5.81ba/s]#015Running tokenizer on dataset:  56%|█████▋    | 222/393 [00:37<00:28,  5.91ba/s]#015Running tokenizer on dataset:  57%|█████▋    | 223/393 [00:37<00:28,  6.00ba/s]#015Running tokenizer on dataset:  57%|█████▋    | 224/393 [00:37<00:28,  6.03ba/s]#015Running tokenizer on dataset:  57%|█████▋    | 225/393 [00:38<00:27,  6.04ba/s]#015Running tokenizer on dataset:  58%|█████▊    | 226/393 [00:38<00:27,  6.08ba/s]#015Running tokenizer on dataset:  58%|█████▊    | 227/393 [00:38<00:27,  6.09ba/s]#015Running tokenizer on dataset:  58%|█████▊    | 228/393 [00:38<00:27,  6.09ba/s]#015Running tokenizer on dataset:  58%|█████▊    | 229/393 [00:38<00:26,  6.11ba/s]#015Running tokenizer on dataset:  59%|█████▊    | 230/393 [00:38<00:26,  6.10ba/s]#015Running tokenizer on dataset:  59%|█████▉    | 231/393 [00:38<00:26,  6.06ba/s]#015Running tokenizer on dataset:  59%|█████▉    | 232/393 [00:39<00:28,  5.64ba/s]#015Running tokenizer on dataset:  59%|█████▉    | 233/393 [00:39<00:27,  5.84ba/s]#015Running tokenizer on dataset:  60%|█████▉    | 234/393 [00:39<00:26,  5.98ba/s]#015Running tokenizer on dataset:  60%|█████▉    | 235/393 [00:39<00:26,  6.04ba/s]#015Running tokenizer on dataset:  60%|██████    | 236/393 [00:39<00:25,  6.10ba/s]#015Running tokenizer on dataset:  60%|██████    | 237/393 [00:39<00:25,  6.10ba/s]#015Running tokenizer on dataset:  61%|██████    | 238/393 [00:40<00:25,  6.10ba/s]#015Running tokenizer on dataset:  61%|██████    | 239/393 [00:40<00:25,  6.10ba/s]#015Running tokenizer on dataset:  61%|██████    | 240/393 [00:40<00:25,  6.06ba/s]#015Running tokenizer on dataset:  61%|██████▏   | 241/393 [00:40<00:25,  6.04ba/s]#015Running tokenizer on dataset:  62%|██████▏   | 242/393 [00:40<00:24,  6.05ba/s]#015Running tokenizer on dataset:  62%|██████▏   | 243/393 [00:40<00:25,  6.00ba/s]#015Running tokenizer on dataset:  62%|██████▏   | 244/393 [00:41<00:26,  5.62ba/s]#015Running tokenizer on dataset:  62%|██████▏   | 245/393 [00:41<00:25,  5.78ba/s]#015Running tokenizer on dataset:  63%|██████▎   | 246/393 [00:41<00:24,  5.92ba/s]#015Running tokenizer on dataset:  63%|██████▎   | 247/393 [00:41<00:24,  6.01ba/s]#015Running tokenizer on dataset:  63%|██████▎   | 248/393 [00:41<00:23,  6.08ba/s]#015Running tokenizer on dataset:  63%|██████▎   | 249/393 [00:41<00:23,  6.11ba/s]#015Running tokenizer on dataset:  64%|██████▎   | 250/393 [00:42<00:23,  6.11ba/s]#015Running tokenizer on dataset:  64%|██████▍   | 251/393 [00:42<00:23,  6.09ba/s]#015Running tokenizer on dataset:  64%|██████▍   | 252/393 [00:42<00:23,  6.09ba/s]#015Running tokenizer on dataset:  64%|██████▍   | 253/393 [00:42<00:23,  6.08ba/s]#015Running tokenizer on dataset:  65%|██████▍   | 254/393 [00:42<00:22,  6.09ba/s]#015Running tokenizer on dataset:  65%|██████▍   | 255/393 [00:42<00:22,  6.08ba/s]#015Running tokenizer on dataset:  65%|██████▌   | 256/393 [00:43<00:24,  5.67ba/s]#015Running tokenizer on dataset:  65%|██████▌   | 257/393 [00:43<00:23,  5.84ba/s]#015Running tokenizer on dataset:  66%|██████▌   | 258/393 [00:43<00:22,  5.96ba/s]#015Running tokenizer on dataset:  66%|██████▌   | 259/393 [00:43<00:22,  6.04ba/s]#015Running tokenizer on dataset:  66%|██████▌   | 260/393 [00:43<00:21,  6.11ba/s]#015Running tokenizer on dataset:  66%|██████▋   | 261/393 [00:43<00:21,  6.13ba/s]#015Running tokenizer on dataset:  67%|██████▋   | 262/393 [00:44<00:21,  6.11ba/s]#015Running tokenizer on dataset:  67%|██████▋   | 263/393 [00:44<00:21,  6.10ba/s]#015Running tokenizer on dataset:  67%|██████▋   | 264/393 [00:44<00:21,  6.09ba/s]#015Running tokenizer on dataset:  67%|██████▋   | 265/393 [00:44<00:21,  6.07ba/s]#015Running tokenizer on dataset:  68%|██████▊   | 266/393 [00:44<00:21,  6.03ba/s]#015Running tokenizer on dataset:  68%|██████▊   | 267/393 [00:44<00:21,  5.99ba/s]#015Running tokenizer on dataset:  68%|██████▊   | 268/393 [00:45<00:22,  5.61ba/s]#015Running tokenizer on dataset:  68%|██████▊   | 269/393 [00:45<00:21,  5.78ba/s]#015Running tokenizer on dataset:  69%|██████▊   | 270/393 [00:45<00:20,  5.94ba/s]#015Running tokenizer on dataset:  69%|██████▉   | 271/393 [00:45<00:20,  6.02ba/s]#015Running tokenizer on dataset:  69%|██████▉   | 272/393 [00:45<00:19,  6.09ba/s]#015Running tokenizer on dataset:  69%|██████▉   | 273/393 [00:45<00:19,  6.15ba/s]#015Running tokenizer on dataset:  70%|██████▉   | 274/393 [00:46<00:19,  6.15ba/s]#015Running tokenizer on dataset:  70%|██████▉   | 275/393 [00:46<00:19,  6.15ba/s]#015Running tokenizer on dataset:  70%|███████   | 276/393 [00:46<00:19,  6.15ba/s]#015Running tokenizer on dataset:  70%|███████   | 277/393 [00:46<00:18,  6.14ba/s]#015Running tokenizer on dataset:  71%|███████   | 278/393 [00:46<00:18,  6.14ba/s]#015Running tokenizer on dataset:  71%|███████   | 279/393 [00:46<00:18,  6.10ba/s]#015Running tokenizer on dataset:  71%|███████   | 280/393 [00:47<00:19,  5.66ba/s]#015Running tokenizer on dataset:  72%|███████▏  | 281/393 [00:47<00:19,  5.83ba/s]#015Running tokenizer on dataset:  72%|███████▏  | 282/393 [00:47<00:18,  5.95ba/s]#015Running tokenizer on dataset:  72%|███████▏  | 283/393 [00:47<00:18,  5.99ba/s]#015Running tokenizer on dataset:  72%|███████▏  | 284/393 [00:47<00:18,  6.04ba/s]#015Running tokenizer on dataset:  73%|███████▎  | 285/393 [00:47<00:17,  6.07ba/s]#015Running tokenizer on dataset:  73%|███████▎  | 286/393 [00:48<00:17,  6.10ba/s]#015Running tokenizer on dataset:  73%|███████▎  | 287/393 [00:48<00:17,  6.13ba/s]#015Running tokenizer on dataset:  73%|███████▎  | 288/393 [00:48<00:17,  6.11ba/s]#015Running tokenizer on dataset:  74%|███████▎  | 289/393 [00:48<00:17,  6.09ba/s]#015Running tokenizer on dataset:  74%|███████▍  | 290/393 [00:48<00:16,  6.07ba/s]#015Running tokenizer on dataset:  74%|███████▍  | 291/393 [00:48<00:16,  6.05ba/s]#015Running tokenizer on dataset:  74%|███████▍  | 292/393 [00:49<00:17,  5.64ba/s]#015Running tokenizer on dataset:  75%|███████▍  | 293/393 [00:49<00:17,  5.81ba/s]#015Running tokenizer on dataset:  75%|███████▍  | 294/393 [00:49<00:16,  5.95ba/s]#015Running tokenizer on dataset:  75%|███████▌  | 295/393 [00:49<00:16,  6.00ba/s]#015Running tokenizer on dataset:  75%|███████▌  | 296/393 [00:49<00:16,  6.05ba/s]#015Running tokenizer on dataset:  76%|███████▌  | 297/393 [00:49<00:15,  6.07ba/s]#015Running tokenizer on dataset:  76%|███████▌  | 298/393 [00:50<00:15,  6.10ba/s]#015Running tokenizer on dataset:  76%|███████▌  | 299/393 [00:50<00:15,  6.09ba/s]#015Running tokenizer on dataset:  76%|███████▋  | 300/393 [00:50<00:15,  6.06ba/s]#015Running tokenizer on dataset:  77%|███████▋  | 301/393 [00:50<00:15,  6.05ba/s]#015Running tokenizer on dataset:  77%|███████▋  | 302/393 [00:50<00:15,  6.05ba/s]#015Running tokenizer on dataset:  77%|███████▋  | 303/393 [00:50<00:14,  6.04ba/s]#015Running tokenizer on dataset:  77%|███████▋  | 304/393 [00:51<00:15,  5.63ba/s]#015Running tokenizer on dataset:  78%|███████▊  | 305/393 [00:51<00:15,  5.80ba/s]#015Running tokenizer on dataset:  78%|███████▊  | 306/393 [00:51<00:14,  5.88ba/s]#015Running tokenizer on dataset:  78%|███████▊  | 307/393 [00:51<00:14,  5.96ba/s]#015Running tokenizer on dataset:  78%|███████▊  | 308/393 [00:51<00:14,  6.01ba/s]#015Running tokenizer on dataset:  79%|███████▊  | 309/393 [00:51<00:13,  6.02ba/s]#015Running tokenizer on dataset:  79%|███████▉  | 310/393 [00:52<00:13,  6.02ba/s]#015Running tokenizer on dataset:  79%|███████▉  | 311/393 [00:52<00:13,  6.01ba/s]#015Running tokenizer on dataset:  79%|███████▉  | 312/393 [00:52<00:13,  6.03ba/s]#015Running tokenizer on dataset:  80%|███████▉  | 313/393 [00:52<00:13,  6.02ba/s]#015Running tokenizer on dataset:  80%|███████▉  | 314/393 [00:52<00:13,  5.99ba/s]#015Running tokenizer on dataset:  80%|████████  | 315/393 [00:52<00:12,  6.01ba/s]#015Running tokenizer on dataset:  80%|████████  | 316/393 [00:53<00:13,  5.64ba/s]#015Running tokenizer on dataset:  81%|████████  | 317/393 [00:53<00:13,  5.81ba/s]#015Running tokenizer on dataset:  81%|████████  | 318/393 [00:53<00:12,  5.94ba/s]#015Running tokenizer on dataset:  81%|████████  | 319/393 [00:53<00:12,  6.02ba/s]#015Running tokenizer on dataset:  81%|████████▏ | 320/393 [00:53<00:12,  6.05ba/s]#015Running tokenizer on dataset:  82%|████████▏ | 321/393 [00:54<00:11,  6.10ba/s]#015Running tokenizer on dataset:  82%|████████▏ | 322/393 [00:54<00:11,  6.12ba/s]#015Running tokenizer on dataset:  82%|████████▏ | 323/393 [00:54<00:11,  6.09ba/s]#015Running tokenizer on dataset:  82%|████████▏ | 324/393 [00:54<00:11,  6.08ba/s]#015Running tokenizer on dataset:  83%|████████▎ | 325/393 [00:54<00:11,  6.06ba/s]#015Running tokenizer on dataset:  83%|████████▎ | 326/393 [00:54<00:11,  6.06ba/s]#015Running tokenizer on dataset:  83%|████████▎ | 327/393 [00:54<00:10,  6.03ba/s]#015Running tokenizer on dataset:  83%|████████▎ | 328/393 [00:55<00:11,  5.58ba/s]#015Running tokenizer on dataset:  84%|████████▎ | 329/393 [00:55<00:11,  5.76ba/s]#015Running tokenizer on dataset:  84%|████████▍ | 330/393 [00:55<00:10,  5.89ba/s]#015Running tokenizer on dataset:  84%|████████▍ | 331/393 [00:55<00:10,  5.95ba/s]#015Running tokenizer on dataset:  84%|████████▍ | 332/393 [00:55<00:10,  6.01ba/s]#015Running tokenizer on dataset:  85%|████████▍ | 333/393 [00:56<00:09,  6.05ba/s]#015Running tokenizer on dataset:  85%|████████▍ | 334/393 [00:56<00:09,  6.03ba/s]#015Running tokenizer on dataset:  85%|████████▌ | 335/393 [00:56<00:09,  6.04ba/s]#015Running tokenizer on dataset:  85%|████████▌ | 336/393 [00:56<00:09,  6.07ba/s]#015Running tokenizer on dataset:  86%|████████▌ | 337/393 [00:56<00:09,  6.09ba/s]#015Running tokenizer on dataset:  86%|████████▌ | 338/393 [00:56<00:09,  6.07ba/s]#015Running tokenizer on dataset:  86%|████████▋ | 339/393 [00:57<00:08,  6.03ba/s]#015Running tokenizer on dataset:  87%|████████▋ | 340/393 [00:57<00:09,  5.65ba/s]#015Running tokenizer on dataset:  87%|████████▋ | 341/393 [00:57<00:08,  5.82ba/s]#015Running tokenizer on dataset:  87%|████████▋ | 342/393 [00:57<00:08,  5.93ba/s]#015Running tokenizer on dataset:  87%|████████▋ | 343/393 [00:57<00:08,  5.99ba/s]#015Running tokenizer on dataset:  88%|████████▊ | 344/393 [00:57<00:08,  6.05ba/s]#015Running tokenizer on dataset:  88%|████████▊ | 345/393 [00:58<00:07,  6.09ba/s]#015Running tokenizer on dataset:  88%|████████▊ | 346/393 [00:58<00:07,  6.07ba/s]#015Running tokenizer on dataset:  88%|████████▊ | 347/393 [00:58<00:07,  6.10ba/s]#015Running tokenizer on dataset:  89%|████████▊ | 348/393 [00:58<00:07,  6.11ba/s]#015Running tokenizer on dataset:  89%|████████▉ | 349/393 [00:58<00:07,  6.08ba/s]#015Running tokenizer on dataset:  89%|████████▉ | 350/393 [00:58<00:07,  6.08ba/s]#015Running tokenizer on dataset:  89%|████████▉ | 351/393 [00:59<00:06,  6.04ba/s]#015Running tokenizer on dataset:  90%|████████▉ | 352/393 [00:59<00:07,  5.63ba/s]#015Running tokenizer on dataset:  90%|████████▉ | 353/393 [00:59<00:06,  5.81ba/s]#015Running tokenizer on dataset:  90%|█████████ | 354/393 [00:59<00:06,  5.91ba/s]#015Running tokenizer on dataset:  90%|█████████ | 355/393 [00:59<00:06,  6.00ba/s]#015Running tokenizer on dataset:  91%|█████████ | 356/393 [00:59<00:06,  6.02ba/s]#015Running tokenizer on dataset:  91%|█████████ | 357/393 [01:00<00:05,  6.05ba/s]#015Running tokenizer on dataset:  91%|█████████ | 358/393 [01:00<00:05,  6.04ba/s]#015Running tokenizer on dataset:  91%|█████████▏| 359/393 [01:00<00:05,  6.05ba/s]#015Running tokenizer on dataset:  92%|█████████▏| 360/393 [01:00<00:05,  6.04ba/s]#015Running tokenizer on dataset:  92%|█████████▏| 361/393 [01:00<00:05,  6.03ba/s]#015Running tokenizer on dataset:  92%|█████████▏| 362/393 [01:00<00:05,  6.01ba/s]#015Running tokenizer on dataset:  92%|█████████▏| 363/393 [01:01<00:05,  6.00ba/s]#015Running tokenizer on dataset:  93%|█████████▎| 364/393 [01:01<00:06,  4.53ba/s]#015Running tokenizer on dataset:  \u001b[0m\n",
      "\u001b[34m93%|█████████▎| 365/393 [01:01<00:05,  4.90ba/s]#015Running tokenizer on dataset:  93%|█████████▎| 366/393 [01:01<00:05,  5.21ba/s]#015Running tokenizer on dataset:  93%|█████████▎| 367/393 [01:01<00:04,  5.45ba/s]#015Running tokenizer on dataset:  94%|█████████▎| 368/393 [01:02<00:04,  5.64ba/s]#015Running tokenizer on dataset:  94%|█████████▍| 369/393 [01:02<00:04,  5.76ba/s]#015Running tokenizer on dataset:  94%|█████████▍| 370/393 [01:02<00:03,  5.86ba/s]#015Running tokenizer on dataset:  94%|█████████▍| 371/393 [01:02<00:03,  5.93ba/s]#015Running tokenizer on dataset:  95%|█████████▍| 372/393 [01:02<00:03,  5.95ba/s]#015Running tokenizer on dataset:  95%|█████████▍| 373/393 [01:02<00:03,  5.97ba/s]#015Running tokenizer on dataset:  95%|█████████▌| 374/393 [01:03<00:03,  5.98ba/s]#015Running tokenizer on dataset:  95%|█████████▌| 375/393 [01:03<00:03,  5.97ba/s]#015Running tokenizer on dataset:  96%|█████████▌| 376/393 [01:03<00:03,  5.59ba/s]#015Running tokenizer on dataset:  96%|█████████▌| 377/393 [01:03<00:02,  5.81ba/s]#015Running tokenizer on dataset:  96%|█████████▌| 378/393 [01:03<00:02,  5.92ba/s]#015Running tokenizer on dataset:  96%|█████████▋| 379/393 [01:03<00:02,  6.01ba/s]#015Running tokenizer on dataset:  97%|█████████▋| 380/393 [01:04<00:02,  6.05ba/s]#015Running tokenizer on dataset:  97%|█████████▋| 381/393 [01:04<00:01,  6.06ba/s]#015Running tokenizer on dataset:  97%|█████████▋| 382/393 [01:04<00:01,  6.06ba/s]#015Running tokenizer on dataset:  97%|█████████▋| 383/393 [01:04<00:01,  6.08ba/s]#015Running tokenizer on dataset:  98%|█████████▊| 384/393 [01:04<00:01,  6.05ba/s]#015Running tokenizer on dataset:  98%|█████████▊| 385/393 [01:04<00:01,  6.06ba/s]#015Running tokenizer on dataset:  98%|█████████▊| 386/393 [01:05<00:01,  6.04ba/s]#015Running tokenizer on dataset:  98%|█████████▊| 387/393 [01:05<00:00,  6.02ba/s]#015Running tokenizer on dataset:  99%|█████████▊| 388/393 [01:05<00:00,  5.62ba/s]#015Running tokenizer on dataset:  99%|█████████▉| 389/393 [01:05<00:00,  5.77ba/s]#015Running tokenizer on dataset:  99%|█████████▉| 390/393 [01:05<00:00,  5.89ba/s]#015Running tokenizer on dataset:  99%|█████████▉| 391/393 [01:05<00:00,  5.95ba/s]#015Running tokenizer on dataset: 100%|█████████▉| 392/393 [01:06<00:00,  6.00ba/s]#015Running tokenizer on dataset: 100%|██████████| 393/393 [01:06<00:00,  6.58ba/s]#015Running tokenizer on dataset: 100%|██████████| 393/393 [01:06<00:00,  5.94ba/s]\u001b[0m\n",
      "\u001b[34m02/12/2022 19:03:16 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bea2ace495a56062.arrow\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015Running tokenizer on dataset:   0%|          | 0/10 [00:00<?, ?ba/s]#015Running tokenizer on dataset:  10%|█         | 1/10 [00:00<00:01,  4.54ba/s]#015Running tokenizer on dataset:  20%|██        | 2/10 [00:00<00:01,  5.30ba/s]#015Running tokenizer on dataset:  30%|███       | 3/10 [00:00<00:01,  5.59ba/s]#015Running tokenizer on dataset:  40%|████      | 4/10 [00:00<00:01,  5.84ba/s]#015Running tokenizer on dataset:  50%|█████     | 5/10 [00:00<00:00,  6.03ba/s]#015Running tokenizer on dataset:  60%|██████    | 6/10 [00:01<00:00,  6.09ba/s]#015Running tokenizer on dataset:  70%|███████   | 7/10 [00:01<00:00,  6.14ba/s]#015Running tokenizer on dataset:  80%|████████  | 8/10 [00:01<00:00,  5.58ba/s]#015Running tokenizer on dataset:  90%|█████████ | 9/10 [00:01<00:00,  5.74ba/s]#015Running tokenizer on dataset: 100%|██████████| 10/10 [00:01<00:00,  6.16ba/s]#015Running tokenizer on dataset: 100%|██████████| 10/10 [00:01<00:00,  5.87ba/s]\u001b[0m\n",
      "\u001b[34m02/12/2022 19:03:17 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cf21e72812d1f64f.arrow\u001b[0m\n",
      "\u001b[34m#015Running tokenizer on dataset:   0%|          | 0/10 [00:00<?, ?ba/s]#015Running tokenizer on dataset:  10%|█         | 1/10 [00:00<00:01,  4.99ba/s]#015Running tokenizer on dataset:  20%|██        | 2/10 [00:00<00:01,  5.51ba/s]#015Running tokenizer on dataset:  30%|███       | 3/10 [00:00<00:01,  5.72ba/s]#015Running tokenizer on dataset:  40%|████      | 4/10 [00:00<00:01,  5.93ba/s]#015Running tokenizer on dataset:  50%|█████     | 5/10 [00:00<00:00,  6.05ba/s]#015Running tokenizer on dataset:  60%|██████    | 6/10 [00:01<00:00,  6.09ba/s]#015Running tokenizer on dataset:  70%|███████   | 7/10 [00:01<00:00,  6.11ba/s]#015Running tokenizer on dataset:  80%|████████  | 8/10 [00:01<00:00,  6.12ba/s]#015Running tokenizer on dataset:  90%|█████████ | 9/10 [00:01<00:00,  6.10ba/s]#015Running tokenizer on dataset: 100%|██████████| 10/10 [00:01<00:00,  5.78ba/s]#015Running tokenizer on dataset: 100%|██████████| 10/10 [00:01<00:00,  5.88ba/s]\u001b[0m\n",
      "\u001b[34m02/12/2022 19:03:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d060d867a5ca363a.arrow\u001b[0m\n",
      "\u001b[34m#015Running tokenizer on dataset:   0%|          | 0/10 [00:00<?, ?ba/s]#015Running tokenizer on dataset:  10%|█         | 1/10 [00:00<00:01,  4.92ba/s]#015Running tokenizer on dataset:  20%|██        | 2/10 [00:00<00:01,  5.52ba/s]#015Running tokenizer on dataset:  30%|███       | 3/10 [00:00<00:01,  5.75ba/s]#015Running tokenizer on dataset:  40%|████      | 4/10 [00:00<00:01,  5.98ba/s]#015Running tokenizer on dataset:  50%|█████     | 5/10 [00:00<00:00,  6.09ba/s]#015Running tokenizer on dataset:  60%|██████    | 6/10 [00:01<00:00,  6.15ba/s]#015Running tokenizer on dataset:  70%|███████   | 7/10 [00:01<00:00,  6.16ba/s]#015Running tokenizer on dataset:  80%|████████  | 8/10 [00:01<00:00,  6.15ba/s]#015Running tokenizer on dataset:  90%|█████████ | 9/10 [00:01<00:00,  6.16ba/s]#015Running tokenizer on dataset: 100%|██████████| 10/10 [00:01<00:00,  6.47ba/s]#015Running tokenizer on dataset: 100%|██████████| 10/10 [00:01<00:00,  6.12ba/s]\u001b[0m\n",
      "\u001b[34m02/12/2022 19:03:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6b1141355089a81a.arrow\u001b[0m\n",
      "\u001b[34m#015Running tokenizer on dataset:   0%|          | 0/10 [00:00<?, ?ba/s]#015Running tokenizer on dataset:  10%|█         | 1/10 [00:00<00:01,  4.92ba/s]#015Running tokenizer on dataset:  20%|██        | 2/10 [00:00<00:01,  4.59ba/s]#015Running tokenizer on dataset:  30%|███       | 3/10 [00:00<00:01,  5.23ba/s]#015Running tokenizer on dataset:  40%|████      | 4/10 [00:00<00:01,  5.59ba/s]#015Running tokenizer on dataset:  50%|█████     | 5/10 [00:00<00:00,  5.80ba/s]#015Running tokenizer on dataset:  60%|██████    | 6/10 [00:01<00:00,  5.95ba/s]#015Running tokenizer on dataset:  70%|███████   | 7/10 [00:01<00:00,  6.02ba/s]#015Running tokenizer on dataset:  80%|████████  | 8/10 [00:01<00:00,  6.06ba/s]#015Running tokenizer on dataset:  90%|█████████ | 9/10 [00:01<00:00,  6.06ba/s]#015Running tokenizer on dataset: 100%|██████████| 10/10 [00:01<00:00,  6.34ba/s]#015Running tokenizer on dataset: 100%|██████████| 10/10 [00:01<00:00,  5.87ba/s]\u001b[0m\n",
      "\u001b[34m02/12/2022 19:03:22 - INFO - __main__ - Sample 335243 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': \"Parents are busy and it's sometimes hard to get them out.\", 'idx': 335243, 'input_ids': [101, 1128, 1221, 1165, 1147, 2153, 1435, 1105, 1122, 112, 188, 1662, 1106, 1243, 1172, 1149, 1105, 170, 1974, 1104, 2153, 1138, 2844, 1106, 1301, 1105, 1105, 1614, 1176, 1115, 1105, 1122, 112, 188, 1523, 1120, 1480, 1177, 102, 24261, 1132, 5116, 1105, 1122, 112, 188, 2121, 1662, 1106, 1243, 1172, 1149, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 0, 'premise': \"you know when their parents come and it's hard to get them out and a lot of parents have places to go and and things like that and it's late at night so\", 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\u001b[0m\n",
      "\u001b[34m02/12/2022 19:03:22 - INFO - __main__ - Sample 58369 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'Where and what is art? ', 'idx': 58369, 'input_ids': [101, 2777, 1110, 1893, 136, 102, 2777, 1105, 1184, 1110, 1893, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'premise': 'Where is art?', 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\u001b[0m\n",
      "\u001b[34m02/12/2022 19:03:22 - INFO - __main__ - Sample 13112 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'hypothesis': 'The list says alcohol and injury are negatives facing staff.', 'idx': 13112, 'input_ids': [101, 2586, 2528, 14084, 1105, 3773, 117, 1112, 1218, 1112, 4094, 22496, 117, 1132, 1113, 1103, 2190, 119, 102, 1109, 2190, 1867, 6272, 1105, 3773, 1132, 4366, 1116, 4749, 2546, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'premise': 'Alcohol and injury, as well as brief interventions, are on the list.', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\u001b[0m\n",
      "\u001b[34m02/12/2022 19:03:23 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.15.1/metrics/glue/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpgpvmozi4\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/1.86k [00:00<?, ?B/s]#015Downloading: 5.78kB [00:00, 5.89MB/s]                   \u001b[0m\n",
      "\u001b[34m02/12/2022 19:03:23 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.15.1/metrics/glue/glue.py in cache at /root/.cache/huggingface/datasets/downloads/52be61f55fbfe81f507fe78e7d5f166999072e90327bb77d1b62da84dae59e11.96f088dbf50cf9762fec7012ded805ef991c57b1aed0fd293b2cee069a9bd5d5.py\u001b[0m\n",
      "\u001b[34m02/12/2022 19:03:23 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/52be61f55fbfe81f507fe78e7d5f166999072e90327bb77d1b62da84dae59e11.96f088dbf50cf9762fec7012ded805ef991c57b1aed0fd293b2cee069a9bd5d5.py\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:540] 2022-02-12 19:03:25,406 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1196] 2022-02-12 19:03:25,414 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1197] 2022-02-12 19:03:25,414 >>   Num examples = 392702\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1198] 2022-02-12 19:03:25,414 >>   Num Epochs = 3\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1199] 2022-02-12 19:03:25,414 >>   Instantaneous batch size per device = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1200] 2022-02-12 19:03:25,414 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1201] 2022-02-12 19:03:25,414 >>   Gradient Accumulation steps = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1202] 2022-02-12 19:03:25,414 >>   Total optimization steps = 36816\u001b[0m\n",
      "\u001b[34m[2022-02-12 19:03:25.498 ip-10-0-129-205.us-east-2.compute.internal:17 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-02-12 19:03:25.534 ip-10-0-129-205.us-east-2.compute.internal:17 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[INFO|trainer.py:540] 2022-02-12 19:06:41,189 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2243] 2022-02-12 19:06:41,191 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2245] 2022-02-12 19:06:41,191 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2248] 2022-02-12 19:06:41,191 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m02/12/2022 19:08:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.7826343178749084, 'eval_accuracy': 0.6681609780947529, 'eval_runtime': 104.5461, 'eval_samples_per_second': 93.882, 'eval_steps_per_second': 11.736, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1995] 2022-02-12 19:08:25,737 >> Saving model checkpoint to /opt/ml/processing/output/checkpoint-200\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:417] 2022-02-12 19:08:25,738 >> Configuration saved in /opt/ml/processing/output/checkpoint-200/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1058] 2022-02-12 19:08:26,423 >> Model weights saved in /opt/ml/processing/output/checkpoint-200/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2034] 2022-02-12 19:08:26,423 >> tokenizer config file saved in /opt/ml/processing/output/checkpoint-200/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2040] 2022-02-12 19:08:26,423 >> Special tokens file saved in /opt/ml/processing/output/checkpoint-200/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:540] 2022-02-12 19:11:43,884 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2243] 2022-02-12 19:11:43,886 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2245] 2022-02-12 19:11:43,886 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2248] 2022-02-12 19:11:43,886 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m02/12/2022 19:13:28 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6925986409187317, 'eval_accuracy': 0.7096281202241467, 'eval_runtime': 104.541, 'eval_samples_per_second': 93.887, 'eval_steps_per_second': 11.737, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1995] 2022-02-12 19:13:28,427 >> Saving model checkpoint to /opt/ml/processing/output/checkpoint-400\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:417] 2022-02-12 19:13:28,428 >> Configuration saved in /opt/ml/processing/output/checkpoint-400/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1058] 2022-02-12 19:13:29,097 >> Model weights saved in /opt/ml/processing/output/checkpoint-400/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2034] 2022-02-12 19:13:29,098 >> tokenizer config file saved in /opt/ml/processing/output/checkpoint-400/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2040] 2022-02-12 19:13:29,098 >> Special tokens file saved in /opt/ml/processing/output/checkpoint-400/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.8147, 'learning_rate': 1.9728378965667103e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:540] 2022-02-12 19:16:46,865 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2243] 2022-02-12 19:16:46,867 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2245] 2022-02-12 19:16:46,867 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2248] 2022-02-12 19:16:46,867 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m02/12/2022 19:18:30 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.661795437335968, 'eval_accuracy': 0.7307182883341824, 'eval_runtime': 103.9298, 'eval_samples_per_second': 94.439, 'eval_steps_per_second': 11.806, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1995] 2022-02-12 19:18:30,797 >> Saving model checkpoint to /opt/ml/processing/output/checkpoint-600\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:417] 2022-02-12 19:18:30,798 >> Configuration saved in /opt/ml/processing/output/checkpoint-600/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1058] 2022-02-12 19:18:31,462 >> Model weights saved in /opt/ml/processing/output/checkpoint-600/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2034] 2022-02-12 19:18:31,462 >> tokenizer config file saved in /opt/ml/processing/output/checkpoint-600/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2040] 2022-02-12 19:18:31,463 >> Special tokens file saved in /opt/ml/processing/output/checkpoint-600/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:540] 2022-02-12 19:21:49,490 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2243] 2022-02-12 19:21:49,492 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2245] 2022-02-12 19:21:49,492 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2248] 2022-02-12 19:21:49,492 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m02/12/2022 19:23:33 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.650580108165741, 'eval_accuracy': 0.733367294956699, 'eval_runtime': 104.403, 'eval_samples_per_second': 94.011, 'eval_steps_per_second': 11.753, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1995] 2022-02-12 19:23:33,896 >> Saving model checkpoint to /opt/ml/processing/output/checkpoint-800\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:417] 2022-02-12 19:23:33,896 >> Configuration saved in /opt/ml/processing/output/checkpoint-800/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1058] 2022-02-12 19:23:34,574 >> Model weights saved in /opt/ml/processing/output/checkpoint-800/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2034] 2022-02-12 19:23:34,575 >> tokenizer config file saved in /opt/ml/processing/output/checkpoint-800/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2040] 2022-02-12 19:23:34,575 >> Special tokens file saved in /opt/ml/processing/output/checkpoint-800/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.6721, 'learning_rate': 1.9456757931334205e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:540] 2022-02-12 19:26:52,261 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2243] 2022-02-12 19:26:52,263 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2245] 2022-02-12 19:26:52,264 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2248] 2022-02-12 19:26:52,264 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m02/12/2022 19:28:37 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6259767413139343, 'eval_accuracy': 0.744982170147733, 'eval_runtime': 105.279, 'eval_samples_per_second': 93.228, 'eval_steps_per_second': 11.655, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1995] 2022-02-12 19:28:37,543 >> Saving model checkpoint to /opt/ml/processing/output/checkpoint-1000\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:417] 2022-02-12 19:28:37,544 >> Configuration saved in /opt/ml/processing/output/checkpoint-1000/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1058] 2022-02-12 19:28:38,237 >> Model weights saved in /opt/ml/processing/output/checkpoint-1000/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2034] 2022-02-12 19:28:38,237 >> tokenizer config file saved in /opt/ml/processing/output/checkpoint-1000/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2040] 2022-02-12 19:28:38,237 >> Special tokens file saved in /opt/ml/processing/output/checkpoint-1000/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:540] 2022-02-12 19:31:56,508 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2243] 2022-02-12 19:31:56,510 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2245] 2022-02-12 19:31:56,510 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2248] 2022-02-12 19:31:56,510 >>   Batch size = 8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m02/12/2022 19:33:41 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.58695387840271, 'eval_accuracy': 0.7615894039735099, 'eval_runtime': 105.4892, 'eval_samples_per_second': 93.043, 'eval_steps_per_second': 11.632, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1995] 2022-02-12 19:33:41,999 >> Saving model checkpoint to /opt/ml/processing/output/checkpoint-1200\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:417] 2022-02-12 19:33:42,000 >> Configuration saved in /opt/ml/processing/output/checkpoint-1200/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1058] 2022-02-12 19:33:42,693 >> Model weights saved in /opt/ml/processing/output/checkpoint-1200/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2034] 2022-02-12 19:33:42,694 >> tokenizer config file saved in /opt/ml/processing/output/checkpoint-1200/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2040] 2022-02-12 19:33:42,694 >> Special tokens file saved in /opt/ml/processing/output/checkpoint-1200/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:540] 2022-02-12 19:37:01,263 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2243] 2022-02-12 19:37:01,265 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2245] 2022-02-12 19:37:01,265 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2248] 2022-02-12 19:37:01,265 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m02/12/2022 19:38:46 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.5810338258743286, 'eval_accuracy': 0.7631176770249618, 'eval_runtime': 105.3052, 'eval_samples_per_second': 93.205, 'eval_steps_per_second': 11.652, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1995] 2022-02-12 19:38:46,571 >> Saving model checkpoint to /opt/ml/processing/output/checkpoint-1400\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:417] 2022-02-12 19:38:46,572 >> Configuration saved in /opt/ml/processing/output/checkpoint-1400/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1058] 2022-02-12 19:38:47,260 >> Model weights saved in /opt/ml/processing/output/checkpoint-1400/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2034] 2022-02-12 19:38:47,261 >> tokenizer config file saved in /opt/ml/processing/output/checkpoint-1400/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2040] 2022-02-12 19:38:47,261 >> Special tokens file saved in /opt/ml/processing/output/checkpoint-1400/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.6279, 'learning_rate': 1.9185136897001307e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:540] 2022-02-12 19:42:05,994 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2243] 2022-02-12 19:42:05,996 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2245] 2022-02-12 19:42:05,996 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2248] 2022-02-12 19:42:05,996 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m02/12/2022 19:43:51 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.5977177619934082, 'eval_accuracy': 0.7592460519612837, 'eval_runtime': 105.863, 'eval_samples_per_second': 92.714, 'eval_steps_per_second': 11.59, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1995] 2022-02-12 19:43:51,859 >> Saving model checkpoint to /opt/ml/processing/output/checkpoint-1600\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:417] 2022-02-12 19:43:51,860 >> Configuration saved in /opt/ml/processing/output/checkpoint-1600/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1058] 2022-02-12 19:43:52,543 >> Model weights saved in /opt/ml/processing/output/checkpoint-1600/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2034] 2022-02-12 19:43:52,544 >> tokenizer config file saved in /opt/ml/processing/output/checkpoint-1600/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2040] 2022-02-12 19:43:52,544 >> Special tokens file saved in /opt/ml/processing/output/checkpoint-1600/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:540] 2022-02-12 19:47:11,489 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2243] 2022-02-12 19:47:11,491 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2245] 2022-02-12 19:47:11,491 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2248] 2022-02-12 19:47:11,492 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m02/12/2022 19:48:57 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.5497726202011108, 'eval_accuracy': 0.7799286805909322, 'eval_runtime': 105.5144, 'eval_samples_per_second': 93.02, 'eval_steps_per_second': 11.629, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1995] 2022-02-12 19:48:57,006 >> Saving model checkpoint to /opt/ml/processing/output/checkpoint-1800\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:417] 2022-02-12 19:48:57,007 >> Configuration saved in /opt/ml/processing/output/checkpoint-1800/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1058] 2022-02-12 19:48:57,702 >> Model weights saved in /opt/ml/processing/output/checkpoint-1800/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2034] 2022-02-12 19:48:57,703 >> tokenizer config file saved in /opt/ml/processing/output/checkpoint-1800/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2040] 2022-02-12 19:48:57,703 >> Special tokens file saved in /opt/ml/processing/output/checkpoint-1800/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m{'loss': 0.5811, 'learning_rate': 1.8913515862668405e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:540] 2022-02-12 19:52:16,201 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2243] 2022-02-12 19:52:16,203 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2245] 2022-02-12 19:52:16,203 >>   Num examples = 9815\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2248] 2022-02-12 19:52:16,203 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m02/12/2022 19:54:01 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/glue/mnli/default_experiment-1-0.arrow\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.5414294600486755, 'eval_accuracy': 0.7835965359144167, 'eval_runtime': 105.4242, 'eval_samples_per_second': 93.1, 'eval_steps_per_second': 11.639, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:1995] 2022-02-12 19:54:01,628 >> Saving model checkpoint to /opt/ml/processing/output/checkpoint-2000\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:417] 2022-02-12 19:54:01,629 >> Configuration saved in /opt/ml/processing/output/checkpoint-2000/config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1058] 2022-02-12 19:54:02,321 >> Model weights saved in /opt/ml/processing/output/checkpoint-2000/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2034] 2022-02-12 19:54:02,321 >> tokenizer config file saved in /opt/ml/processing/output/checkpoint-2000/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2040] 2022-02-12 19:54:02,322 >> Special tokens file saved in /opt/ml/processing/output/checkpoint-2000/special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sm_local_input_models = \"/opt/ml/processing/input/data/models\"\n",
    "sm_local_input_data = \"/opt/ml/processing/input/data/jsonlines\"\n",
    "sm_local_input_vocab = \"/opt/ml/processing/input/data/vocab\"\n",
    "\n",
    "\n",
    "sm_local_output = \"/opt/ml/processing/output\"\n",
    "\n",
    "\n",
    "\n",
    "# python run_glue.py \\\n",
    "#   --model_name_or_path bert-base-cased \\\n",
    "#   --task_name $TASK_NAME \\\n",
    "#   --do_train \\\n",
    "#   --do_eval \\\n",
    "#   --max_seq_length 128 \\\n",
    "#   --per_device_train_batch_size 32 \\\n",
    "#   --learning_rate 2e-5 \\\n",
    "#   --num_train_epochs 3 \\\n",
    "#   --output_dir /tmp/$TASK_NAME/\n",
    "\n",
    "\n",
    "script_processor.run(\n",
    "        code=f'run_glue.py',\n",
    "        source_dir=f'{transformer_examples_dir}/examples/pytorch/text-classification',\n",
    "        arguments=[\n",
    "            \"--task_name\", \"mnli\",\n",
    "            \"--model_name_or_path\", \"bert-base-cased\",\n",
    "            \"--do_train\", \"1\",\n",
    "            \"--do_eval\",\"1\",\n",
    "            \"--do_predict\",\"1\",\n",
    "            \"--max_seq_length\", str(512),\n",
    "            \"--per_device_train_batch_size\", str(8),\n",
    "            \"--gradient_accumulation_steps\", str(4),\n",
    "            \"--learning_rate\", str(2e-5),\n",
    "            \"--num_train_epochs\", str(3),\n",
    "            \"--output_dir\", sm_local_output,\n",
    "            \"--overwrite_output_dir\", \"1\",\n",
    "            \"--load_best_model_at_end\", \"1\",     # load the best model when finished training (default metric is loss)\n",
    "            \"--eval_steps\",\"200\",\n",
    "            \"--save_steps\",\"200\",\n",
    "            \"--evaluation_strategy\",\"steps\",\n",
    "            \"--disable_tqdm\",\"1\"\n",
    "           \n",
    "        ],\n",
    "\n",
    "        inputs=[\n",
    "#                 ProcessingInput(\n",
    "#                     source=s3_input_data,\n",
    "#                     s3_data_type = s3_data_type,\n",
    "#                     destination=sm_local_input_data,\n",
    "#                     s3_data_distribution_type=\"FullyReplicated\"),\n",
    "\n",
    "#                 ProcessingInput(\n",
    "#                         source=s3_model_path,\n",
    "#                         destination=sm_local_input_models,\n",
    "#                         s3_data_distribution_type=\"FullyReplicated\"),\n",
    "\n",
    "#                 ProcessingInput(\n",
    "#                         source=s3_input_vocab,\n",
    "#                         destination=sm_local_input_vocab,\n",
    "#                         s3_data_distribution_type=\"FullyReplicated\")\n",
    "            ],\n",
    "\n",
    "\n",
    "        outputs=[ProcessingOutput(\n",
    "                source=sm_local_output, \n",
    "                destination=s3_output_path,\n",
    "                output_name='predictions')]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from uuid import uuid4\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "\n",
    "#role = sagemaker.get_execution_role()\n",
    "role=\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20190118T115449\".format(account_id)\n",
    "max_runs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = \"temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_examples_dir = os.path.join(temp_dir, \"hugging_face_example\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup image and instance type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image_name=f\"huggingface-pytorch-training:1.9.1-transformers4.12.3-gpu-py38-cu111-ubuntu20.04\"\n",
    "image_account_id=\"763104351884\"\n",
    "instance_type = \"ml.p3.2xlarge\"\n",
    "instance_type_gpu_map = {\"ml.p3.8xlarge\":4, \"ml.p3.2xlarge\": 1, \"ml.p3.16xlarge\":8}\n",
    "instance_count=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = \"{}.dkr.ecr.{}.amazonaws.com/{}\".format(image_account_id, region, custom_image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure train/ test and validation datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"aegovan-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert=\"s3://{}/embeddings/bert_base_cased/\".format(bucket)\n",
    "\n",
    "\n",
    "trainfile = \"s3://{}/glue_dataset/train/multinli_1.0_train.jsonl\".format(bucket)\n",
    "# valfile=\"s3://{}/mnli_dataset/val/multinli_1.0_dev_matched.jsonl\".format(bucket)\n",
    "\n",
    "#trainfile = \"s3://{}/mnli_dataset_mini/train/multinli.jsonl\".format(bucket)\n",
    "valfile=\"s3://{}/glue_dataset_mini/train/multinli.jsonl\".format(bucket)\n",
    "\n",
    "s3_output_path= \"s3://{}/glue_sagemakerresults/\".format(bucket)\n",
    "s3_code_path= \"s3://{}/glue_code\".format(bucket)\n",
    "s3_checkpoint = \"s3://{}/mnli_bert_checkpoint/{}\".format(bucket, str(uuid4()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run processing job training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(transformer_examples_dir):\n",
    "    shutil.rmtree(transformer_examples_dir)\n",
    "    os.makedirs(transformer_examples_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'temp/hugging_face_example'...\n",
      "remote: Enumerating objects: 99654, done.\u001b[K\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
      "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
      "remote: Total 99654 (delta 7), reused 17 (delta 5), pack-reused 99631\u001b[K\n",
      "Receiving objects: 100% (99654/99654), 84.61 MiB | 2.65 MiB/s, done.\n",
      "Resolving deltas: 100% (72298/72298), done.\n",
      "Note: switching to 'tags/v4.12.3'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 3ea15d278 Style\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers $transformer_examples_dir\n",
    "!git -C $transformer_examples_dir checkout tags/v4.12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "\n",
    "script_processor = FrameworkProcessor(HuggingFace,\n",
    "                                      framework_version=None,\n",
    "                                      image_uri=docker_repo,\n",
    "                                      code_location = s3_code_path, \n",
    "                                       py_version=\"py36\",\n",
    "                                       command=[\"python\"],\n",
    "                                       env={'mode': 'python', 'PYTHONPATH':'/opt/ml/code'},\n",
    "                                       role=role,\n",
    "                                       instance_type=instance_type,\n",
    "                                       instance_count=instance_count,\n",
    "                                       max_runtime_in_seconds= 5 * 24 * 60 * 60,\n",
    "                                       volume_size_in_gb = 250,\n",
    "                                       network_config=NetworkConfig(enable_network_isolation=False),\n",
    "                                       base_job_name =\"glue-processing\"\n",
    "                                       )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  glue-processing-2022-02-07-15-40-59-233\n",
      "Inputs:  [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/glue_code/glue-processing-2022-02-07-15-40-59-233/source/sourcedir.tar.gz', 'LocalPath': '/opt/ml/processing/input/code/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'entrypoint', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aegovan-data/glue_code/glue-processing-2022-02-07-15-40-59-233/source/runproc.sh', 'LocalPath': '/opt/ml/processing/input/entrypoint', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'predictions', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://aegovan-data/glue_sagemakerresults/', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".......................................\u001b[34mWARNING: Skipping typing as it is not installed.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mCollecting accelerate\n",
      "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.8.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (0.1.96)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (0.24.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (3.19.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from accelerate->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from accelerate->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.70.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (21.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (5.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.62.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2021.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (1.2.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 5)) (2.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.10.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets>=1.8.0->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (1.26.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2021.10.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (21.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (5.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: accelerate\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.5.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:39 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:39 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=True,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_steps=None,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=IntervalStrategy.NO,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=None,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_strategy=HubStrategy.EVERY_SAVE,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=2e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=False,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=-1,\u001b[0m\n",
      "\u001b[34mlog_level_replica=-1,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/processing/output/runs/Feb07_15-47-39_ip-10-0-130-85.us-east-2.compute.internal,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=None,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=3.0,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/processing/output,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=32,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/processing/output,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=500,\u001b[0m\n",
      "\u001b[34msave_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:40 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.15.1/datasets/glue/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpvmosyiix\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s]#015Downloading: 28.8kB [00:00, 20.2MB/s]                   \u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:40 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.15.1/datasets/glue/glue.py in cache at /root/.cache/huggingface/datasets/downloads/ebcebc40af3c6b9af1a2f380ea06637ee192bce2d17d528809dd0ee2fa281675.759f3e257a3fad0984d9f8ba9a26479d341795eb50fa64e4c1de40f1fc421313.py\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:40 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/ebcebc40af3c6b9af1a2f380ea06637ee192bce2d17d528809dd0ee2fa281675.759f3e257a3fad0984d9f8ba9a26479d341795eb50fa64e4c1de40f1fc421313.py\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:40 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/1.15.1/datasets/glue/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpugjnq4oc\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]#015Downloading: 28.7kB [00:00, 24.1MB/s]                   \u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:40 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/1.15.1/datasets/glue/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/eea1163a0dd089739f6e5e3951d74b1200c7e66d462d607c43cbc8c4e69bbd47.082d8848abcb8cddda90647ec069014ca338abd4f45e0a83c6df1ece0d45476a\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:40 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/eea1163a0dd089739f6e5e3951d74b1200c7e66d462d607c43cbc8c4e69bbd47.082d8848abcb8cddda90647ec069014ca338abd4f45e0a83c6df1ece0d45476a\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:40 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:40 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset glue/mnli (download: 298.29 MiB, generated: 78.65 MiB, post-processed: Unknown size, total: 376.95 MiB) to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:40 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:41 - INFO - datasets.utils.file_utils - https://dl.fbaipublicfiles.com/glue/data/MNLI.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpumtd6a0f\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015Downloading:   0%|          | 0.00/313M [00:00<?, ?B/s]#015Downloading:   0%|          | 4.10k/313M [00:00<2:08:31, 40.6kB/s]#015Downloading:   0%|          | 56.3k/313M [00:00<17:59, 290kB/s]   #015Downloading:   0%|          | 257k/313M [00:00<05:07, 1.02MB/s]#015Downloading:   0%|          | 362k/313M [00:00<06:59, 745kB/s] #015Downloading:   0%|          | 1.21M/313M [00:00<01:50, 2.82MB/s]#015Downloading:   1%|          | 2.70M/313M [00:00<00:49, 6.21MB/s]#015Downloading:   2%|▏         | 5.11M/313M [00:00<00:27, 11.3MB/s]#015Downloading:   3%|▎         | 8.46M/313M [00:00<00:17, 17.6MB/s]#015Downloading:   4%|▍         | 12.7M/313M [00:01<00:12, 24.6MB/s]#015Downloading:   5%|▌         | 16.5M/313M [00:01<00:10, 28.3MB/s]#015Downloading:   7%|▋         | 20.7M/313M [00:01<00:09, 31.8MB/s]#015Downloading:   8%|▊         | 24.8M/313M [00:01<00:08, 34.1MB/s]#015Downloading:   9%|▉         | 28.9M/313M [00:01<00:07, 35.5MB/s]#015Downloading:  11%|█         | 33.0M/313M [00:01<00:07, 36.7MB/s]#015Downloading:  12%|█▏        | 37.2M/313M [00:01<00:07, 37.4MB/s]#015Downloading:  13%|█▎        | 41.4M/313M [00:01<00:07, 37.8MB/s]#015Downloading:  15%|█▍        | 45.6M/313M [00:01<00:06, 39.0MB/s]#015Downloading:  16%|█▌        | 49.7M/313M [00:01<00:06, 39.4MB/s]#015Downloading:  17%|█▋        | 53.9M/313M [00:02<00:06, 39.7MB/s]#015Downloading:  19%|█▊        | 58.1M/313M [00:02<00:06, 39.4MB/s]#015Downloading:  20%|█▉        | 62.3M/313M [00:02<00:06, 39.7MB/s]#015Downloading:  21%|██▏       | 66.5M/313M [00:02<00:06, 39.9MB/s]#015Downloading:  23%|██▎       | 70.7M/313M [00:02<00:06, 40.3MB/s]#015Downloading:  24%|██▍       | 74.7M/313M [00:02<00:05, 40.4MB/s]#015Downloading:  25%|██▌       | 79.0M/313M [00:02<00:05, 39.1MB/s]#015Downloading:  27%|██▋       | 83.3M/313M [00:02<00:05, 39.6MB/s]#015Downloading:  28%|██▊       | 87.4M/313M [00:02<00:05, 39.9MB/s]#015Downloading:  29%|██▉       | 91.4M/313M [00:03<00:05, 39.9MB/s]#015Downloading:  31%|███       | 95.5M/313M [00:03<00:05, 39.9MB/s]#015Downloading:  32%|███▏      | 99.7M/313M [00:03<00:05, 39.8MB/s]#015Downloading:  33%|███▎      | 104M/313M [00:03<00:05, 40.1MB/s] #015Downloading:  34%|███▍      | 108M/313M [00:03<00:05, 39.6MB/s]#015Downloading:  36%|███▌      | 112M/313M [00:03<00:05, 39.7MB/s]#015Downloading:  37%|███▋      | 116M/313M [00:03<00:04, 39.4MB/s]#015Downloading:  38%|███▊      | 120M/313M [00:03<00:04, 40.0MB/s]#015Downloading:  40%|███▉      | 125M/313M [00:03<00:04, 40.0MB/s]#015Downloading:  41%|████      | 129M/313M [00:03<00:04, 39.8MB/s]#015Downloading:  42%|████▏     | 133M/313M [00:04<00:04, 40.0MB/s]#015Downloading:  44%|████▍     | 137M/313M [00:04<00:04, 40.1MB/s]#015Downloading:  45%|████▌     | 141M/313M [00:04<00:04, 39.9MB/s]#015Downloading:  46%|████▋     | 145M/313M [00:04<00:04, 40.3MB/s]#015Downloading:  48%|████▊     | 150M/313M [00:04<00:04, 39.9MB/s]#015Downloading:  49%|████▉     | 154M/313M [00:04<00:03, 40.1MB/s]#015Downloading:  51%|█████     | 158M/313M [00:04<00:03, 40.0MB/s]#015Downloading:  52%|█████▏    | 162M/313M [00:04<00:03, 40.2MB/s]#015Downloading:  53%|█████▎    | 166M/313M [00:04<00:03, 40.0MB/s]#015Downloading:  55%|█████▍    | 171M/313M [00:05<00:03, 40.2MB/s]#015Downloading:  56%|█████▌    | 175M/313M [00:05<00:03, 40.0MB/s]#015Downloading:  57%|█████▋    | 179M/313M [00:05<00:03, 40.2MB/s]#015Downloading:  59%|█████▊    | 183M/313M [00:05<00:03, 39.7MB/s]#015Downloading:  60%|█████▉    | 187M/313M [00:05<00:03, 39.8MB/s]#015Downloading:  61%|██████    | 191M/313M [00:05<00:03, 40.2MB/s]#015Downloading:  63%|██████▎   | 196M/313M [00:05<00:02, 39.7MB/s]#015Downloading:  64%|██████▍   | 200M/313M [00:05<00:02, 39.3MB/s]#015Downloading:  65%|██████▌   | 204M/313M [00:05<00:02, 40.2MB/s]#015Downloading:  67%|██████▋   | 208M/313M [00:05<00:02, 40.2MB/s]#015Downloading:  68%|██████▊   | 212M/313M [00:06<00:02, 39.9MB/s]#015Downloading:  69%|██████▉   | 217M/313M [00:06<00:02, 40.3MB/s]#015Downloading:  71%|███████   | 221M/313M [00:06<00:02, 39.8MB/s]#015Downloading:  72%|███████▏  | 225M/313M [00:06<00:02, 40.0MB/s]#015Downloading:  73%|███████▎  | 229M/313M [00:06<00:02, 39.6MB/s]#015Downloading:  75%|███████▍  | 233M/313M [00:06<00:01, 40.3MB/s]#015Downloading:  76%|███████▌  | 237M/313M [00:06<00:01, 40.3MB/s]#015Downloading:  77%|███████▋  | 242M/313M [00:06<00:01, 38.9MB/s]#015Downloading:  79%|███████▊  | 246M/313M [00:06<00:01, 39.8MB/s]#015Downloading:  80%|███████▉  | 250M/313M [00:06<00:01, 39.9MB/s]#015Downloading:  81%|████████▏ | 254M/313M [00:07<00:01, 40.1MB/s]#015Downloading:  83%|████████▎ | 258M/313M [00:07<00:01, 40.3MB/s]#015Downloading:  84%|████████▍ | 263M/313M [00:07<00:01, 40.7MB/s]#015Downloading:  85%|████████▌ | 267M/313M [00:07<00:01, 39.9MB/s]#015Downloading:  87%|████████▋ | 271M/313M [00:07<00:01, 39.9MB/s]#015Downloading:  88%|████████▊ | 275M/313M [00:07<00:00, 40.3MB/s]#015Downloading:  89%|████████▉ | 279M/313M [00:07<00:00, 40.5MB/s]#015Downloading:  91%|█████████ | 283M/313M [00:07<00:00, 40.0MB/s]#015Downloading:  92%|█████████▏| 288M/313M [00:07<00:00, 40.3MB/s]#015Downloading:  93%|█████████▎| 292M/313M [00:08<00:00, 40.3MB/s]#015Downloading:  95%|█████████▍| 296M/313M [00:08<00:00, 39.9MB/s]#015Downloading:  96%|█████████▌| 300M/313M [00:08<00:00, 39.0MB/s]#015Downloading:  97%|█████████▋| 304M/313M [00:08<00:00, 40.1MB/s]#015Downloading:  99%|█████████▊| 308M/313M [00:08<00:00, 40.4MB/s]#015Downloading: 100%|█████████▉| 313M/313M [00:08<00:00, 40.0MB/s]#015Downloading: 100%|██████████| 313M/313M [00:08<00:00, 36.5MB/s]\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:50 - INFO - datasets.utils.file_utils - storing https://dl.fbaipublicfiles.com/glue/data/MNLI.zip in cache at /root/.cache/huggingface/datasets/downloads/74d7bc70ada44c1086d1ba81cf6271c128514f629fb8edcd548c113939e3b5f2\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:50 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/74d7bc70ada44c1086d1ba81cf6271c128514f629fb8edcd548c113939e3b5f2\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:50 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:51 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:59 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\u001b[0m\n",
      "\u001b[34m02/07/2022 15:47:59 - INFO - datasets.builder - Generating split train\u001b[0m\n",
      "\u001b[34m02/07/2022 15:48:18 - INFO - datasets.builder - Generating split validation_matched\u001b[0m\n",
      "\u001b[34m02/07/2022 15:48:18 - INFO - datasets.builder - Generating split validation_mismatched\u001b[0m\n",
      "\u001b[34m02/07/2022 15:48:19 - INFO - datasets.builder - Generating split test_matched\u001b[0m\n",
      "\u001b[34m02/07/2022 15:48:19 - INFO - datasets.builder - Generating split test_mismatched\u001b[0m\n",
      "\u001b[34m02/07/2022 15:48:20 - INFO - datasets.utils.info_utils - All the splits matched successfully.\u001b[0m\n",
      "\u001b[34mDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m#0150 examples [00:00, ? examples/s]#0152076 examples [00:00, 20748.54 examples/s]#0154267 examples [00:00, 21427.34 examples/s]#0156454 examples [00:00, 21628.11 examples/s]#0158621 examples [00:00, 21639.32 examples/s]#01510785 examples [00:00, 20615.37 examples/s]#01513012 examples [00:00, 21157.29 examples/s]#01515242 examples [00:00, 21520.02 examples/s]#01517437 examples [00:00, 21651.57 examples/s]#01519629 examples [00:00, 21731.84 examples/s]#01521805 examples [00:01, 21063.37 examples/s]#01523931 examples [00:01, 21118.64 examples/s]#01526048 examples [00:01, 20799.85 examples/s]#01528132 examples [00:01, 20513.14 examples/s]#01530187 examples [00:01, 16156.01 examples/s]#01532363 examples [00:01, 17547.04 examples/s]#01534543 examples [00:01, 18658.70 examples/s]#01536645 examples [00:01, 19299.19 examples/s]#01538822 examples [00:01, 19988.60 examples/s]#01540883 examples [00:02, 19675.56 examples/s]#01543019 examples [00:02, 20152.98 examples/s]#01545208 examples [00:02, 20654.84 examples/s]#01547360 examples [00:02, 20903.91 examples/s]#01549536 examples [00:02, 21154.40 examples/s]#01551665 examples [00:02, 20699.30 examples/s]#01553867 examples [00:02, 21085.15 examples/s]#01556061 examples [00:02, 21336.56 examples/s]#01558243 examples [00:02, 21477.96 examples/s]#01560396 examples [00:02, 20875.98 examples/s]#01562587 examples [00:03, 21177.43 examples/s]#01564771 examples [00:03, 21371.68 examples/s]#01566955 examples [00:03, 21509.21 examples/s]#01569133 examples [00:03, 21587.11 examples/s]#01571294 examples [00:03, 20869.45 examples/s]#01573505 examples [00:03, 21228.81 examples/s]#01575703 examples [00:03, 21448.41 examples/s]#01577883 examples [00:03, 21549.75 examples/s]#01580042 examples [00:03, 20897.64 examples/s]#01582242 examples [00:03, 21217.08 examples/s]#01584369 examples [00:04, 17824.53 examples/s]#01586562 examples [00:04, 18893.38 examples/s]#01588735 examples [00:04, 19662.52 examples/s]#01590767 examples [00:04, 19493.18 examples/s]#01592965 examples [00:04, 20192.31 examples/s]#01595164 examples [00:04, 20707.39 examples/s]#01597362 examples [00:04, 21076.60 examples/s]#01599541 examples [00:04, 21285.56 examples/s]#015101685 examples [00:04, 20754.80 examples/s]#015103894 examples [00:05, 21138.87 examples/s]#015106091 examples [00:05, 21381.34 examples/s]#015108281 examples [00:05, 21532.36 examples/s]#015110440 examples [00:05, 20914.23 examples/s]#015112539 examples [00:05, 20814.38 examples/s]#015114667 examples [00:05, 20950.33 examples/s]#015116856 examples [00:05, 21225.82 examples/s]#015119030 examples [00:05, 21377.68 examples/s]#015121171 examples [00:05, 20759.16 examples/s]#015123367 examples [00:05, 21109.17 examples/s]#015125562 examples [00:06, 21353.60 examples/s]#015127767 examples [00:06, 21557.57 examples/s]#015129945 examples [00:06, 21621.01 examples/s]#015132110 examples [00:06, 21048.83 examples/s]#015134323 examples [00:06, 21365.14 examples/s]#015136464 examples [00:06, 18161.11 examples/s]#015138638 examples [00:06, 19102.50 examples/s]#015140620 examples [00:06, 19188.33 examples/s]#015142823 examples [00:06, 19982.92 examples/s]#015144920 examples [00:07, 20262.49 examples/s]#015147125 examples [00:07, 20776.48 examples/s]#015149309 examples [00:07, 21086.21 examples/s]#015151435 examples [00:07, 20578.78 examples/s]#015153615 examples [00:07, 20933.11 examples/s]#015155793 examples [00:07, 21180.12 examples/s]#015157981 examples [00:07, 21384.77 examples/s]#015160126 examples [00:07, 20732.23 examples/s]#015162335 examples [00:07, 21125.72 examples/s]#015164518 examples [00:07, 21329.58 examples/s]#015166685 examples [00:08, 21429.81 examples/s]#015168892 examples [00:08, 21616.20 examples/s]#015171057 examples [00:08, 20755.28 examples/s]#015173271 examples [00:08, 21155.06 examples/s]#015175464 examples [00:08, 21380.44 examples/s]#015177653 examples [00:08, 21527.75 examples/s]#015179843 examples [00:08, 21636.89 examples/s]#015182010 examples [00:08, 20822.61 examples/s]#015184210 examples [00:08, 21161.88 examples/s]#015186425 examples [00:09, 21450.52 examples/s]#015188576 examples [00:09, 18376.20 examples/s]#015190493 examples [00:09, 18552.85 examples/s]#015192701 examples [00:09, 19519.60 examples/s]#015194919 examples [00:09, 20266.72 examples/s]#015197131 examples [00:09, 20796.50 examples/s]#015199337 examples [00:09, 21161.77 examples/s]#015201476 examples [00:09, 20762.15 examples/s]#015203569 examples [00:09, 20780.66 examples/s]#015205671 examples [00:09, 20848.51 examples/s]#015207878 examples [00:10, 21208.24 examples/s]#015210006 examples [00:10, 20748.79 examples/s]#015212218 examples [00:10, 21148.69 examples/s]#015214422 examples [00:10, 21410.52 examples/s]#015216621 examples [00:10, 21581.20 examples/s]#015218821 examples [00:10, 21704.11 examples/s]#015220994 examples [00:10, 21040.42 examples/s]#015223192 examples [00:10, 21313.20 examples/s]#015225384 examples [00:10, 21490.43 examples/s]#015227570 examples [00:10, 21598.86 examples/s]#015229759 examples [00:11, 21682.41 examples/s]#015231930 examples [00:11, 21074.02 examples/s]#015234138 examples [00:11, 21367.62 examples/s]#015236341 examples [00:11, 21560.77 examples/s]#015238532 examples [00:11, 21661.93 examples/s]#015240701 examples [00:11, 20615.45 examples/s]#015242917 examples [00:11, 21056.76 examples/s]#015245033 examples [00:11, 17260.63 examples/s]#015247195 examples [00:11, 18367.87 examples/s]#015249359 examples [00:12, 19238.08 examples/s]#015251365 examples [00:12, 19235.37 examples/s]#015253553 examples [00:12, 19974.93 examples/s]#015255707 examples [00:12, 20419.53 examples/s]#015257903 examples [00:12, 20864.99 examples/s]#015260015 examples [00:12, 20325.82 examples/s]#015262227 examples [00:12, 20842.84 examples/s]#015264432 examples [00:12, 21192.02 examples/s]#015266629 examples [00:12, 21418.46 examples/s]#015268824 examples [00:13, 21573.63 examples/s]#015270988 examples [00:13, 20965.15 examples/s]#015273202 examples [00:13, 21304.29 examples/s]#015275339 examples [00:13, 21254.92 examples/s]#015277469 examples [00:13, 21130.29 examples/s]#015279669 examples [00:13, 21386.62 examples/s]#015281811 examples [00:13, 20727.19 examples/s]#015283931 examples [00:13, 20862.39 examples/s]#015286022 examples [00:13, 20734.41 examples/s]#015288099 examples [00:13, 20580.93 examples/s]#015290160 examples [00:14, 19935.99 examples/s]#015292378 examples [00:14, 20583.51 examples/s]#015294586 examples [00:14, 21018.74 examples/s]#015296694 examples [00:14, 17765.78 examples/s]#015298878 examples [00:14, 18832.88 examples/s]#015300838 examples [00:14, 18913.91 examples/s]#015303037 examples [00:14, 19769.77 examples/s]#015305212 examples [00:14, 20331.96 examples/s]#015307395 examples [00:14, 20762.55 examples/s]#015309569 examples [00:15, 21046.16 examples/s]#015311693 examples [00:15, 20571.51 examples/s]#015313892 examples [00:15, 20983.04 examples/s]#015316002 examples [00:15, 20727.00 examples/s]#015318084 examples [00:15, 15073.79 examples/s]#015320000 examples [00:15, 15552.28 examples/s]#015321988 examples [00:15, 16607.81 examples/s]#015324162 examples [00:15, 17933.18 examples/s]#015326345 examples [00:15, 18982.33 examples/s]#015328529 examples [00:16, 19775.62 examples/s]#015330580 examples [00:16, 19642.15 examples/s]#015332779 examples [00:16, 20310.66 examples/s]#015334946 examples [00:16, 20702.67 examples/s]#015337128 examples [00:16, 21026.85 examples/s]#015339315 examples [00:16, 21272.68 examples/s]#015341458 examples [00:16, 20603.85 examples/s]#015343640 examples [00:16, 20954.89 examples/s]#015345822 examples [00:16, 21207.43 examples/s]#015348015 examples [00:16, 21418.00 examples/s]#015350164 examples [00:17, 16727.29 examples/s]#015352338 examples [00:17, 17970.48 examples/s]#015354489 examples [00:17, 18896.45 examples/s]#015356656 examples [00:17, 19648.75 examples/s]#015358844 examples [00:17, 20272.04 examples/s]#015360936 examples [00:17, 19967.87 examples/s]#015363076 examples [00:17, 20373.95 examples/s]#015365254 examples [00:17, 20780.22 examples/s]#015367404 examples [00:17, 20988.96 examples/s]#015369599 examples [00:18, 21271.13 examples/s]#015371740 examples [00:18, 20688.37 examples/s]#015373910 examples [00:18, 20981.48 examples/s]#015376081 examples [00:18, 21194.31 examples/s]#015378278 examples [00:18, 21420.38 examples/s]#015380426 examples [00:18, 20514.88 examples/s]#015382623 examples [00:18, 20932.28 examples/s]#015384801 examples [00:18, 21177.83 examples/s]#015386987 examples [00:18, 21375.90 examples/s]#015389171 examples [00:19, 21509.71 examples/s]#015391326 examples [00:19, 20851.67 examples/s]#015                                            #015#0150 examples [00:00, ? examples/s]#0152083 examples [00:00, 20827.24 examples/s]#0154230 examples [00:00, 21201.76 examples/s]#0156385 examples [00:00, 21356.88 examples/s]#0158521 examples [00:00, 17012.47 examples/s]#015                                          #015#0150 examples [00:00, ? examples/s]#0152049 examples [00:00, 20485.28 examples/s]#0154185 examples [00:00, 20997.16 examples/s]#0156312 examples [00:00, 21120.41 examples/s]#0158435 examples [00:00, 21162.88 examples/s]#015                                          #015#0150 examples [00:00, ? examples/s]#0152308 examples [00:00, 23078.26 examples/s]#0154694 examples [00:00, 23532.17 examples/s]#0157075 examples [00:00, 23656.01 examples/s]#0159459 examples [00:00, 23724.94 examples/s]#015                                          #015#0150 examples [00:00, ? examples/s]#0152277 examples [00:00, 22756.40 examples/s]#0154635 examples [00:00, 23239.14 examples/s]#0156997 examples [00:00, 23411.65 examples/s]#0159339 examples [00:00, 23380.34 examples/s]#015                                          #015#015  0%|          | 0/5 [00:00<?, ?it/s]#015100%|██████████| 5/5 [00:00<00:00, 810.24it/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1753] 2022-02-07 15:48:20,236 >> https://huggingface.co/bert-base-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphygrcqst\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 570/570 [00:00<00:00, 872kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1757] 2022-02-07 15:48:20,302 >> storing https://huggingface.co/bert-base-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1765] 2022-02-07 15:48:20,302 >> creating metadata file for /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:588] 2022-02-07 15:48:20,302 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:625] 2022-02-07 15:48:20,303 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"mnli\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1753] 2022-02-07 15:48:20,370 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpp_dvs3vq\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 33.7kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1757] 2022-02-07 15:48:20,437 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1765] 2022-02-07 15:48:20,438 >> creating metadata file for /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:588] 2022-02-07 15:48:20,504 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:625] 2022-02-07 15:48:20,504 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1753] 2022-02-07 15:48:20,641 >> https://huggingface.co/bert-base-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpqob2_d82\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 208k/208k [00:00<00:00, 4.87MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1757] 2022-02-07 15:48:20,752 >> storing https://huggingface.co/bert-base-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1765] 2022-02-07 15:48:20,752 >> creating metadata file for /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1753] 2022-02-07 15:48:20,819 >> https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn2sviajo\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 426k/426k [00:00<00:00, 7.89MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1757] 2022-02-07 15:48:20,943 >> storing https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1765] 2022-02-07 15:48:20,944 >> creating metadata file for /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1742] 2022-02-07 15:48:21,147 >> loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1742] 2022-02-07 15:48:21,147 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1742] 2022-02-07 15:48:21,148 >> loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1742] 2022-02-07 15:48:21,148 >> loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1742] 2022-02-07 15:48:21,148 >> loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:588] 2022-02-07 15:48:21,215 >> loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:625] 2022-02-07 15:48:21,216 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1753] 2022-02-07 15:48:21,339 >> https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpeybpzwm_\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]#015Downloading:   0%|          | 766k/416M [00:00<00:55, 7.82MB/s]#015Downloading:   1%|▏         | 5.89M/416M [00:00<00:12, 34.9MB/s]#015Downloading:   3%|▎         | 11.7M/416M [00:00<00:09, 47.0MB/s]#015Downloading:   4%|▍         | 17.9M/416M [00:00<00:07, 54.0MB/s]#015Downloading:   6%|▌         | 24.2M/416M [00:00<00:07, 58.3MB/s]#015Downloading:   7%|▋         | 30.6M/416M [00:00<00:06, 61.1MB/s]#015Downloading:   9%|▉         | 36.9M/416M [00:00<00:06, 62.8MB/s]#015Downloading:  10%|█         | 43.3M/416M [00:00<00:06, 64.2MB/s]#015Downloading:  12%|█▏        | 49.7M/416M [00:00<00:05, 65.2MB/s]#015Downloading:  13%|█▎        | 56.1M/416M [00:01<00:05, 65.7MB/s]#015Downloading:  15%|█▌        | 62.4M/416M [00:01<00:05, 66.0MB/s]#015Downloading:  17%|█▋        | 68.9M/416M [00:01<00:05, 66.5MB/s]#015Downloading:  18%|█▊        | 75.4M/416M [00:01<00:05, 66.9MB/s]#015Downloading:  20%|█▉        | 81.8M/416M [00:01<00:05, 67.1MB/s]#015Downloading:  21%|██        | 88.2M/416M [00:01<00:05, 67.1MB/s]#015Downloading:  23%|██▎       | 94.6M/416M [00:01<00:05, 67.1MB/s]#015Downloading:  24%|██▍       | 101M/416M [00:01<00:04, 67.0MB/s] #015Downloading:  26%|██▌       | 107M/416M [00:01<00:04, 67.0MB/s]#015Downloading:  27%|██▋       | 114M/416M [00:01<00:04, 67.2MB/s]#015Downloading:  29%|██▉       | 120M/416M [00:02<00:04, 66.5MB/s]#015Downloading:  30%|███       | 127M/416M [00:02<00:04, 66.1MB/s]#015Downloading:  32%|███▏      | 133M/416M [00:02<00:04, 66.4MB/s]#015Downloading:  34%|███▎      | 139M/416M [00:02<00:04, 66.4MB/s]#015Downloading:  35%|███▌      | 146M/416M [00:02<00:04, 67.0MB/s]#015Downloading:  37%|███▋      | 153M/416M [00:02<00:04, 67.9MB/s]#015Downloading:  38%|███▊      | 160M/416M [00:02<00:03, 70.6MB/s]#015Downloading:  40%|████      | 167M/416M [00:02<00:03, 72.7MB/s]#015Downloading:  42%|████▏     | 174M/416M [00:02<00:03, 67.6MB/s]#015Downloading:  43%|████▎     | 181M/416M [00:02<00:03, 66.8MB/s]#015Downloading:  45%|████▌     | 187M/416M [00:03<00:03, 66.1MB/s]#015Downloading:  47%|████▋     | 194M/416M [00:03<00:03, 64.9MB/s]#015Downloading:  48%|████▊     | 200M/416M [00:03<00:04, 55.6MB/s]#015Downloading:  49%|████▉     | 205M/416M [00:03<00:04, 50.7MB/s]#015Downloading:  51%|█████     | 211M/416M [00:03<00:03, 53.8MB/s]#015Downloading:  52%|█████▏    | 217M/416M [00:03<00:03, 56.4MB/s]#015Downloading:  54%|█████▍    | 224M/416M [00:03<00:03, 59.2MB/s]#015Downloading:  55%|█████▌    | 230M/416M [00:03<00:03, 61.4MB/s]#015Downloading:  57%|█████▋    | 237M/416M [00:03<00:02, 63.1MB/s]#015Downloading:  58%|█████▊    | 243M/416M [00:04<00:02, 64.3MB/s]#015Downloading:  60%|█████▉    | 249M/416M [00:04<00:02, 65.2MB/s]#015Downloading:  62%|██████▏   | 256M/416M [00:04<00:02, 65.4MB/s]#015Downloading:  63%|██████▎   | 262M/416M [00:04<00:02, 66.0MB/s]#015Downloading:  65%|██████▍   | 268M/416M [00:04<00:02, 66.2MB/s]#015Downloading:  66%|██████▌   | 275M/416M [00:04<00:02, 66.4MB/s]#015Downloading:  68%|██████▊   | 281M/416M [00:04<00:02, 66.1MB/s]#015Downloading:  69%|██████▉   | 287M/416M [00:04<00:02, 66.0MB/s]#015Downloading:  71%|███████   | 294M/416M [00:04<00:01, 66.3MB/s]#015Downloading:  72%|███████▏  | 300M/416M [00:04<00:01, 66.6MB/s]#015Downloading:  74%|███████▍  | 307M/416M [00:05<00:01, 66.9MB/s]#015Downloading:  75%|███████▌  | 313M/416M [00:05<00:01, 66.9MB/s]#015Downloading:  77%|███████▋  | 319M/416M [00:05<00:01, 66.9MB/s]#015Downloading:  78%|███████▊  | 326M/416M [00:05<00:01, 66.9MB/s]#015Downloading:  80%|███████▉  | 332M/416M [00:05<00:01, 67.2MB/s]#015Downloading:  82%|████████▏ | 339M/416M [00:05<00:01, 62.0MB/s]#015Downloading:  83%|████████▎ | 345M/416M [00:05<00:01, 62.9MB/s]#015Downloading:  85%|████████▍ | 351M/416M [00:05<00:01, 63.9MB/s]#015Downloading:  86%|████████▌ | 358M/416M [00:05<00:00, 64.7MB/s]#015Downloading:  88%|████████▊ | 364M/416M [00:05<00:00, 65.3MB/s]#015Downloading:  89%|████████▉ | 370M/416M [00:06<00:00, 65.6MB/s]#015Downloading:  91%|█████████ | 377M/416M [00:06<00:00, 65.9MB/s]#015Downloading:  92%|█████████▏| 383M/416M [00:06<00:00, 66.4MB/s]#015Downloading:  94%|█████████▎| 389M/416M [00:06<00:00, 66.5MB/s]#015Downloading:  95%|█████████▌| 396M/416M [00:06<00:00, 66.5MB/s]#015Downloading:  97%|█████████▋| 402M/416M [00:06<00:00, 67.0MB/s]#015Downloading:  98%|█████████▊| 409M/416M [00:06<00:00, 67.2MB/s]#015Downloading: 100%|█████████▉| 415M/416M [00:06<00:00, 67.9MB/s]#015Downloading: 100%|██████████| 416M/416M [00:06<00:00, 64.2MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1757] 2022-02-07 15:48:28,279 >> storing https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:1765] 2022-02-07 15:48:28,280 >> creating metadata file for /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1340] 2022-02-07 15:48:28,280 >> loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1598] 2022-02-07 15:48:30,094 >> Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1609] 2022-02-07 15:48:30,094 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m02/07/2022 15:48:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-48413cedb88f4fab.arrow\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sm_local_input_models = \"/opt/ml/processing/input/data/models\"\n",
    "sm_local_input_data = \"/opt/ml/processing/input/data/jsonlines\"\n",
    "sm_local_input_vocab = \"/opt/ml/processing/input/data/vocab\"\n",
    "\n",
    "\n",
    "sm_local_output = \"/opt/ml/processing/output\"\n",
    "\n",
    "\n",
    "\n",
    "# python run_glue.py \\\n",
    "#   --model_name_or_path bert-base-cased \\\n",
    "#   --task_name $TASK_NAME \\\n",
    "#   --do_train \\\n",
    "#   --do_eval \\\n",
    "#   --max_seq_length 128 \\\n",
    "#   --per_device_train_batch_size 32 \\\n",
    "#   --learning_rate 2e-5 \\\n",
    "#   --num_train_epochs 3 \\\n",
    "#   --output_dir /tmp/$TASK_NAME/\n",
    "\n",
    "\n",
    "script_processor.run(\n",
    "        code=f'run_glue.py',\n",
    "        source_dir=f'{transformer_examples_dir}/examples/pytorch/text-classification',\n",
    "        arguments=[\n",
    "            \"--task_name\", \"mnli\",\n",
    "            \"--model_name_or_path\", \"bert-base-cased\",\n",
    "            \"--do_train\", \"1\",\n",
    "            \"--do_eval\",\"1\",\n",
    "            \"--do_predict\",\"1\",\n",
    "            \"--max_seq_length\", str(512),\n",
    "            \"--per_device_train_batch_size\", str(32),\n",
    "            \"--learning_rate\", str(2e-5),\n",
    "            \"--num_train_epochs\", str(3),\n",
    "            \"--output_dir\", sm_local_output\n",
    "           \n",
    "        ],\n",
    "\n",
    "        inputs=[\n",
    "#                 ProcessingInput(\n",
    "#                     source=s3_input_data,\n",
    "#                     s3_data_type = s3_data_type,\n",
    "#                     destination=sm_local_input_data,\n",
    "#                     s3_data_distribution_type=\"FullyReplicated\"),\n",
    "\n",
    "#                 ProcessingInput(\n",
    "#                         source=s3_model_path,\n",
    "#                         destination=sm_local_input_models,\n",
    "#                         s3_data_distribution_type=\"FullyReplicated\"),\n",
    "\n",
    "#                 ProcessingInput(\n",
    "#                         source=s3_input_vocab,\n",
    "#                         destination=sm_local_input_vocab,\n",
    "#                         s3_data_distribution_type=\"FullyReplicated\")\n",
    "            ],\n",
    "\n",
    "\n",
    "        outputs=[ProcessingOutput(\n",
    "                source=sm_local_output, \n",
    "                destination=s3_output_path,\n",
    "                output_name='predictions')]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
